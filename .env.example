# ========================================
# RAG ПАЙПЛАЙН - КОНФИГУРАЦИЯ ОКРУЖЕНИЯ
# ========================================
# Скопируйте этот файл в .env и заполните необходимые значения:
#   cp .env.example .env

# ========================================
# ЛОГИРОВАНИЕ
# ========================================
LOG_LEVEL=INFO
LOG_FILE=pipeline.log

# ========================================
# LLM НАСТРОЙКИ (ОБЯЗАТЕЛЬНО!)
# ========================================
# Режим работы LLM: "local" (локальная модель через llama-cpp) или "api" (OpenRouter API)
LLM_MODE=api

# === API РЕЖИМ (OpenRouter) ===
# ВАЖНО: OpenRouter требует ключ даже для бесплатных моделей
# Получите бесплатный ключ: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# Модель для использования (примеры бесплатных моделей):
# ⭐ РЕКОМЕНДУЕТСЯ:
# - tngtech/tng-r1t-chimera:free (163k context, enhanced tool-calling, reasoning)
# - qwen/qwen3-coder:free (262k context, 480B MoE, специализирована для кода)
# - openai/gpt-oss-20b:free (131k context, 21B, function calling)
#
# Другие бесплатные:
# - tngtech/deepseek-r1t2-chimera:free (163k context, старая версия)
# - openrouter/sherlock-think-alpha (1.8M context, reasoning)
# - meta-llama/llama-3.2-3b-instruct:free (маленькая)
#
# Полный список: https://openrouter.ai/models
LLM_API_MODEL=tngtech/tng-r1t-chimera:free

# Опционально: провайдер для роутинга (grok, openai, anthropic, и т.д.)
LLM_API_ROUTING=

# Параметры API
LLM_API_MAX_WORKERS=10
LLM_API_TIMEOUT=60
LLM_API_RETRIES=3
LLM_API_MAX_TOKENS=150000  # Increased for tng-r1t-chimera (max 163840)

# ========================================
# БАЗЫ ДАННЫХ
# ========================================

# === NEO4J (ГРАФ ЗНАНИЙ) ===
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=code_rag

# === WEAVIATE (ВЕКТОРНАЯ БД) ===
USE_WEAVIATE=true
WEAVIATE_URL=http://localhost:8080

# ========================================
# ОБРАБОТКА ДАННЫХ
# ========================================
# Размер chunk'ов при чтении CSV (маленький для LLM)
CSV_CHUNKSIZE=10

# Размер chunk'ов для подсчета документов (большой для скорости)
CSV_COUNT_CHUNKSIZE=50000

# Количество параллельных воркеров для LLM обработки (1-4)
# ВНИМАНИЕ: требует много VRAM! Для A100 80GB можно 2-3
LLM_PARALLEL_WORKERS=1

# Количество параллельных воркеров для обработки вопросов (1-20)
# Рекомендуется 5-10 для API режима
QUESTION_PROCESSING_WORKERS=20

# Принудительное использование CPU вместо GPU
# FORCE_CPU=false

# ========================================
# RAG УЛУЧШЕНИЯ (ФЛАГИ)
# ========================================
# Reciprocal Rank Fusion (объединение Dense + BM25)
ENABLE_RRF=true

# Context Window (добавление соседних чанков)
ENABLE_CONTEXT_WINDOW=true

# Query Expansion (расширение запроса синонимами)
ENABLE_QUERY_EXPANSION=false

# Metadata Filtering (фильтрация по метаданным)
ENABLE_METADATA_FILTER=true

# Usefulness Filtering (фильтрация по полезности документов)
ENABLE_USEFULNESS_FILTER=true

# Dynamic TOP_K (адаптивный выбор количества результатов)
ENABLE_DYNAMIC_TOP_K=true

# ========================================
# МОДЕЛИ
# ========================================

# === EMBEDDING MODEL ===
# Модель для генерации векторных представлений
# Локальная модель (скачивается автоматически или через: python scripts/download_models.py)
EMBEDDING_MODEL=BAAI/bge-m3

# === RERANKER MODEL ===
# Тип reranker: "cross_encoder" (быстро), "llm" (качественно, медленно), "none"
RERANKER_TYPE=cross_encoder
# Модель для reranking (если RERANKER_TYPE=cross_encoder)
# Локальная модель (скачивается автоматически или через: python scripts/download_models.py)
# Рекомендуется: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1 (multilingual, русский)
# Альтернатива: cross-encoder/ms-marco-MiniLM-L-6-v2 (только английский, быстрее)
RERANKER_MODEL=cross-encoder/mmarco-mMiniLMv2-L12-H384-v1

# === AGENT MODELS ===
# Code Explorer Agent - итеративное исследование кода
# Рекомендуется: qwen/qwen3-coder:free (262k context, специализирована для кода)
CODE_EXPLORER_MODEL=qwen/qwen3-coder:free

# Query Orchestrator - классификация типов вопросов
ORCHESTRATOR_MODEL=deepseek/deepseek-r1:free

# Analysis Model - для traceback analysis и business agent
# Рекомендуется: tngtech/tng-r1t-chimera:free (enhanced tool-calling)
ANALYSIS_MODEL=tngtech/tng-r1t-chimera:free

# Query Reformulation - переформулирование запросов
QUERY_REFORMULATION_MODEL=tngtech/tng-r1t-chimera:free

# ========================================
# TELEGRAM BOT (ОПЦИОНАЛЬНО)
# ========================================
# Токен для Telegram бота
# Получите у @BotFather: https://t.me/botfather
TELEGRAM_BOT_TOKEN=

# ========================================
# GRID SEARCH ОПТИМИЗАЦИЯ
# ========================================
# Режим grid search: "test" (5 комбинаций), "quick" (54), "full" (1225)
GRID_SEARCH_MODE=test

# Использовать LLM для оценки в grid search (точнее, но медленнее)
GRID_SEARCH_USE_LLM=true

# ========================================
# ПРОДВИНУТЫЕ ФУНКЦИИ (ОПЦИОНАЛЬНО)
# ========================================
# Parent-Child Chunking (улучшенный чанкинг)
# ENABLE_PARENT_CHILD_CHUNKING=false

# Агентный RAG (итеративный поиск)
# ENABLE_AGENT_RAG=false

# ========================================
# БЫСТРЫЙ СТАРТ
# ========================================
# 1. Скопируйте этот файл: cp .env.example .env
# 2. Получите OPENROUTER_API_KEY на https://openrouter.ai/keys
# 3. Вставьте ключ в OPENROUTER_API_KEY=
# 4. (Опционально) Получите TELEGRAM_BOT_TOKEN у @BotFather
# 5. Скачайте локальные модели (embedding + reranker):
#    python scripts/download_models.py
#    Или пропустите - модели скачаются автоматически при первом использовании
# 6. Запустите базы данных: docker-compose up -d
# 7. Проиндексируйте репозиторий:
#    python -m src.code_rag.graph.build_and_index /path/to/repo
# 8. Запустите Telegram бота:
#    python -m src.telegram_bot.bot
#    Или API сервер:
#    uvicorn src.api.main:app --reload
#
# Документация:
# - README.md - общая информация
# - docs/QUICKSTART.md - быстрый старт
# - docs/MODEL_CONFIGURATION.md - настройка моделей
# - docs/MODEL_RECOMMENDATIONS.md - рекомендации по моделям
