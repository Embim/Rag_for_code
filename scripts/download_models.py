"""
ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ A100 80GB
Ğ—Ğ°Ğ¿ÑƒÑĞº: python download_models.py
"""
import os
import subprocess
from pathlib import Path

# Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
MODELS_DIR = Path(__file__).parent / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ
# Ğ•Ğ”Ğ˜ĞĞĞ¯ ĞœĞĞ”Ğ•Ğ›Ğ¬: Qwen3-32B Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ (Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ reranking)
MODELS = {
    "embedding": {
        "repo": "BAAI/bge-m3",
        "description": "BGE-M3 embedding Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Ğ»ÑƒÑ‡ÑˆĞ°Ñ multilingual)",
        "size": "~2 GB"
    },
    "llm": {
        "repo": "bartowski/Qwen3-32B-2507-GGUF",
        "files": ["Qwen3-32B-2507-Q8_0.gguf"],
        "description": "Qwen3-32B 8-bit (Ğ•Ğ”Ğ˜ĞĞĞ¯ Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ˜ reranking)",
        "size": "~32 GB"
    }
}

def check_huggingface_cli():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ huggingface-cli"""
    try:
        subprocess.run(["huggingface-cli", "--version"], check=True, capture_output=True)
        print("âœ… huggingface-cli ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ huggingface-cli Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½!")
        print("\nĞ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸:")
        print("  pip install huggingface_hub")
        return False

def download_model(model_name, config):
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    print(f"\n{'='*80}")
    print(f"ğŸ“¥ Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: {model_name}")
    print(f"   Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹: {config['repo']}")
    print(f"   ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: {config['description']}")
    print(f"   Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {config['size']}")
    print(f"{'='*80}\n")

    # Ğ”Ğ»Ñ embedding Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ĞµÑÑŒ Ñ€ĞµĞ¿Ğ¾
    if model_name == "embedding":
        cmd = [
            "huggingface-cli", "download",
            config["repo"],
            "--local-dir", str(MODELS_DIR / "bge-m3")
        ]

    # Ğ”Ğ»Ñ GGUF Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² - Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
    elif "files" in config:
        for file in config["files"]:
            cmd = [
                "huggingface-cli", "download",
                config["repo"],
                file,
                "--local-dir", str(MODELS_DIR)
            ]
            print(f"ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°: {' '.join(cmd)}")
            subprocess.run(cmd, check=True)
        return True

    # Ğ”Ğ»Ñ HuggingFace Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ²ĞµÑÑŒ Ñ€ĞµĞ¿Ğ¾
    else:
        model_dir = MODELS_DIR / config["repo"].split("/")[-1]
        cmd = [
            "huggingface-cli", "download",
            config["repo"],
            "--local-dir", str(model_dir)
        ]

    print(f"ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°: {' '.join(cmd)}")

    try:
        subprocess.run(cmd, check=True)
        print(f"âœ… {model_name} ÑĞºĞ°Ñ‡Ğ°Ğ½!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ {model_name}: {e}")
        return False

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ Ğ”Ğ›Ğ¯ A100 80GB                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° HuggingFace CLI
    if not check_huggingface_cli():
        return

    # Ğ Ğ°ÑÑ‡ĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°
    print("\nğŸ“Š Ğ¢Ñ€ĞµĞ±ÑƒĞµĞ¼Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ½Ğ° Ğ´Ğ¸ÑĞºĞµ:")
    print("   BGE-M3:            ~2 GB")
    print("   Qwen3-32B (8-bit): ~32 GB (Ğ•Ğ”Ğ˜ĞĞĞ¯ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡)")
    print("   " + "-" * 50)
    print("   Ğ˜Ğ¢ĞĞ“Ğ:             ~34 GB")

    confirm = input("\nâš ï¸  ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ? (yes/no): ")
    if confirm.lower() != 'yes':
        print("ĞÑ‚Ğ¼ĞµĞ½ĞµĞ½Ğ¾.")
        return

    # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    success = []
    failed = []

    for model_name, config in MODELS.items():
        print(f"\n[{len(success)+len(failed)+1}/{len(MODELS)}] {model_name}")

        if download_model(model_name, config):
            success.append(model_name)
        else:
            failed.append(model_name)

    # Ğ˜Ñ‚Ğ¾Ğ³Ğ¸
    print("\n" + "="*80)
    print("ğŸ“Š Ğ˜Ğ¢ĞĞ“Ğ˜ Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ¯:")
    print(f"   âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {len(success)}/{len(MODELS)}")
    if success:
        print(f"      {', '.join(success)}")

    if failed:
        print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ¸: {len(failed)}/{len(MODELS)}")
        print(f"      {', '.join(failed)}")

    print("="*80)

    # Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸
    if len(success) == len(MODELS):
        print("\nâœ… Ğ’Ğ¡Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ¡ĞšĞĞ§ĞĞĞ«!")
        print("\nğŸ“‹ Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸:")
        print("1. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸:")
        print("   bash full_pipeline.sh")
        print("\nĞ˜Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¿Ğ¾ ÑˆĞ°Ğ³Ğ°Ğ¼:")
        print("1. ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Weaviate:")
        print("   docker-compose up -d")
        print("\n2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ‡Ğ°Ğ½ĞºĞ¸:")
        print("   python main_pipeline.py chunk")
        print("\n3. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Qwen3-32B:")
        print("   python scripts/preprocess_documents_qwen25.py")
        print("\n4. Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ embeddings:")
        print("   python main_pipeline.py build --input data/processed/chunks_cleaned.csv")
        print("\n5. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ inference:")
        print("   python main_pipeline.py search")
    else:
        print("\nâš ï¸  ĞĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ»Ğ¸ÑÑŒ. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ñ‹ÑˆĞµ.")
        print("ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹: python download_models.py")

if __name__ == "__main__":
    main()
