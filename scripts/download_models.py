"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è Code RAG.

–ú–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏,
–Ω–æ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∫–∞—á–∞—Ç—å –∏—Ö –∑–∞—Ä–∞–Ω–µ–µ.

–ó–∞–ø—É—Å–∫: python scripts/download_models.py
"""
import os
import sys
import subprocess
from pathlib import Path
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env
load_dotenv()

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π
MODELS_DIR = Path(__file__).parent.parent / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
MODELS = {
    "embedding": {
        "repo": os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3"),
        "description": "Embedding –º–æ–¥–µ–ª—å (–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞)",
        "size": "~2 GB",
        "required": True,  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è
        "local": True  # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    },
    "reranker": {
        "repo": os.getenv("RERANKER_MODEL", "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1"),
        "description": "Reranker –º–æ–¥–µ–ª—å (–ø–µ—Ä–µ—Ä–∞–Ω–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)",
        "size": "~500 MB",
        "required": True,  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è
        "local": True  # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    },
    "llm_local": {
        "repo": "unsloth/Qwen3-32B-GGUF",
        "files": ["Qwen3-32B-IQ4_NL.gguf"],
        "description": "–õ–æ–∫–∞–ª—å–Ω–∞—è LLM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ OpenRouter API)",
        "size": "~20 GB",
        "required": False,  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è
        "local": True  # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    }
}

def check_huggingface_cli():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ huggingface-cli"""
    try:
        # –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ python -m (—Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö)
        subprocess.run(
            [sys.executable, "-m", "huggingface_hub.commands.huggingface_cli", "--help"],
            check=True,
            capture_output=True
        )
        print("‚úÖ huggingface-cli —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("‚ùå huggingface-cli –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("\n–£—Å—Ç–∞–Ω–æ–≤–∏:")
        print("  pip install huggingface_hub")
        return False

def download_model(model_name, config):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print(f"\n{'='*80}")
    print(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {model_name}")
    print(f"   –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {config['repo']}")
    print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {config['description']}")
    print(f"   –†–∞–∑–º–µ—Ä: {config['size']}")
    print(f"{'='*80}\n")

    # –ë–∞–∑–æ–≤–∞—è –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
    base_cmd = [sys.executable, "-m", "huggingface_hub.commands.huggingface_cli", "download"]

    # –î–ª—è embedding –º–æ–¥–µ–ª–µ–π - —Å–∫–∞—á–∏–≤–∞–µ–º –≤–µ—Å—å —Ä–µ–ø–æ
    if model_name == "embedding":
        cmd = base_cmd + [
            config["repo"],
            "--local-dir", str(MODELS_DIR / "bge-m3")
        ]

    # –î–ª—è GGUF —Ñ–∞–π–ª–æ–≤ - —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã
    elif "files" in config:
        for file in config["files"]:
            cmd = base_cmd + [
                config["repo"],
                file,
                "--local-dir", str(MODELS_DIR)
            ]
            print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
            subprocess.run(cmd, check=True)
        return True

    # –î–ª—è HuggingFace –º–æ–¥–µ–ª–µ–π - –≤–µ—Å—å —Ä–µ–ø–æ
    else:
        model_dir = MODELS_DIR / config["repo"].split("/")[-1]
        cmd = base_cmd + [
            config["repo"],
            "--local-dir", str(model_dir)
        ]

    print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")

    try:
        subprocess.run(cmd, check=True)
        print(f"‚úÖ {model_name} —Å–∫–∞—á–∞–Ω!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {model_name}: {e}")
        return False

def main():
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                  –°–ö–ê–ß–ò–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô –î–õ–Ø CODE RAG                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ HuggingFace CLI
    if not check_huggingface_cli():
        return

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
    print("\nüì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:\n")
    required_models = {k: v for k, v in MODELS.items() if v.get('required', True)}
    optional_models = {k: v for k, v in MODELS.items() if not v.get('required', True)}

    print("–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï (–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã):")
    total_required_size = 0
    for name, config in required_models.items():
        status = "‚úÖ –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ"
        print(f"  ‚Ä¢ {name}: {config['description']}")
        print(f"    –†–∞–∑–º–µ—Ä: {config['size']}, –ú–æ–¥–µ–ª—å: {config['repo']}")
        # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        if "GB" in config['size']:
            total_required_size += float(config['size'].split('~')[1].split(' ')[0])
        elif "MB" in config['size']:
            total_required_size += float(config['size'].split('~')[1].split(' ')[0]) / 1024

    print(f"\n–û–ü–¶–ò–û–ù–ê–õ–¨–ù–´–ï (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º LLM):")
    for name, config in optional_models.items():
        print(f"  ‚Ä¢ {name}: {config['description']}")
        print(f"    –†–∞–∑–º–µ—Ä: {config['size']}, –ú–æ–¥–µ–ª—å: {config['repo']}")

    # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞
    print(f"\nüìä –¢—Ä–µ–±—É–µ–º–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ:")
    print(f"   –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏:  ~{total_required_size:.1f} GB")
    print(f"   + –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ:       ~20 GB (–µ—Å–ª–∏ –Ω—É–∂–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è LLM)")
    print("   " + "-" * 50)

    print("\n–ß—Ç–æ —Å–∫–∞—á–∏–≤–∞—Ç—å?")
    print("1. –¢–æ–ª—å–∫–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ OpenRouter API)")
    print("2. –í—Å–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º)")
    print("3. –í—ã–±—Ä–∞—Ç—å –≤—Ä—É—á–Ω—É—é")

    choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1/2/3): ").strip()

    models_to_download = {}
    if choice == "1":
        models_to_download = required_models
    elif choice == "2":
        models_to_download = MODELS
    elif choice == "3":
        print("\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (y/n):")
        for name, config in MODELS.items():
            response = input(f"  –°–∫–∞—á–∞—Ç—å {name}? (y/n): ").strip().lower()
            if response == 'y':
                models_to_download[name] = config
    else:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –û—Ç–º–µ–Ω–µ–Ω–æ.")
        return

    if not models_to_download:
        print("‚ùå –ù–µ –≤—ã–±—Ä–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏. –û—Ç–º–µ–Ω–µ–Ω–æ.")
        return

    confirm = input(f"\n‚ö†Ô∏è  –°–∫–∞—á–∞—Ç—å {len(models_to_download)} –º–æ–¥–µ–ª—å(–∏)? (yes/no): ")
    if confirm.lower() != 'yes':
        print("–û—Ç–º–µ–Ω–µ–Ω–æ.")
        return

    # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
    success = []
    failed = []

    for model_name, config in models_to_download.items():
        print(f"\n[{len(success)+len(failed)+1}/{len(models_to_download)}] {model_name}")

        if download_model(model_name, config):
            success.append(model_name)
        else:
            failed.append(model_name)

    # –ò—Ç–æ–≥–∏
    print("\n" + "="*80)
    print("üìä –ò–¢–û–ì–ò –°–ö–ê–ß–ò–í–ê–ù–ò–Ø:")
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {len(success)}/{len(MODELS)}")
    if success:
        print(f"      {', '.join(success)}")

    if failed:
        print(f"   ‚ùå –û—à–∏–±–∫–∏: {len(failed)}/{len(MODELS)}")
        print(f"      {', '.join(failed)}")

    print("="*80)

    # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    if success:
        print(f"\n‚úÖ –°–∫–∞—á–∞–Ω–æ {len(success)} –º–æ–¥–µ–ª—å(–∏)!")
        print("\nüìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ .env –Ω–∞—Å—Ç—Ä–æ–µ–Ω:")
        print("   cp .env.example .env")
        print("   nano .env  # –¥–æ–±–∞–≤—å—Ç–µ OPENROUTER_API_KEY –∏ –¥—Ä—É–≥–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ")
        print("\n2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö:")
        print("   docker-compose up -d")
        print("\n3. –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:")
        print("   python -m src.code_rag.graph.build_and_index /path/to/repo")
        print("\n4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Telegram –±–æ—Ç–∞ –∏–ª–∏ API:")
        print("   python -m src.telegram_bot.bot")
        print("   # –∏–ª–∏")
        print("   uvicorn src.api.main:app --reload")
        print("\nüìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:")
        print("   docs/QUICKSTART.md - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç")
        print("   docs/MODEL_CONFIGURATION.md - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π")

    if failed:
        print("\n‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å–∫–∞—á–∞–ª–∏—Å—å. –ü—Ä–æ–≤–µ—Ä—å –æ—à–∏–±–∫–∏ –≤—ã—à–µ.")
        print("–ú–æ–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∫–æ–º–∞–Ω–¥–æ–π: python scripts/download_models.py")
        print("\n‚ÑπÔ∏è  –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ú–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ —Å–∫–∞—á–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏.")

if __name__ == "__main__":
    main()
