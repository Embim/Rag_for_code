"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç RAG –ø–∞–π–ø–ª–∞–π–Ω–∞

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python main_pipeline.py build           # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    python main_pipeline.py search          # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã
    python main_pipeline.py all             # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª (build + search)
    python main_pipeline.py evaluate        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
"""
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm

from src.config import (
    WEBSITES_CSV,
    QUESTIONS_CSV,
    MODELS_DIR,
    OUTPUTS_DIR,
    PROCESSED_DIR,
    USE_WEAVIATE,
    ENABLE_AGENT_RAG,
    LOG_LEVEL,
    LOG_FILE
)
from src.preprocessing import load_and_preprocess_documents, load_and_preprocess_questions
from src.chunking import create_chunks_from_documents
from src.indexing import WeaviateIndexer
from src.retrieval import RAGPipeline
from src.llm_preprocessing import apply_llm_cleaning
from src.grid_search_optimizer import optimize_rag_params
from src.logger import setup_logging, get_logger, log_timing
import logging
import time

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Weaviate
try:
    import weaviate
    WEAVIATE_AVAILABLE = True
except ImportError:
    WEAVIATE_AVAILABLE = False
    if USE_WEAVIATE:
        # –õ–æ–≥–≥–µ—Ä –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ main()
        pass


def build_knowledge_base(force_rebuild: bool = False, llm_clean: bool = False,
                        min_usefulness: float = 0.3):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (offline —ç—Ç–∞–ø)

    Args:
        force_rebuild: –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        llm_clean: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ)
        min_usefulness: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è LLM —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (0.0-1.0)

    Returns:
        (embedding_indexer, bm25_indexer, chunks_df)
    """
    logger = get_logger(__name__)
    logger.info("="*80)
    logger.info("–≠–¢–ê–ü 1: –ü–û–°–¢–†–û–ï–ù–ò–ï –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (OFFLINE)")
    logger.info("="*80)

    chunks_path = PROCESSED_DIR / "chunks.pkl"

    # –í —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Weaviate
    if not (USE_WEAVIATE and WEAVIATE_AVAILABLE):
        raise RuntimeError("Weaviate –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ weaviate-client –∏ –≤–∫–ª—é—á–∏—Ç–µ USE_WEAVIATE=True –≤ config.py")

    logger.info("[–†–ï–ñ–ò–ú] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ Weaviate –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–≥–∏–±—Ä–∏–¥–Ω—ã–π —Å BM25)")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ª–∏ —á–∞–Ω–∫–∏
    if not force_rebuild and chunks_path.exists():
        logger.info("–ß–∞–Ω–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç. –ó–∞–≥—Ä—É–∂–∞–µ–º...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤
        chunks_df = pd.read_pickle(chunks_path)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks_df)} —á–∞–Ω–∫–æ–≤")

        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Weaviate
        try:
            weaviate_indexer = WeaviateIndexer()
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            weaviate_indexer.chunk_metadata = chunks_df

            logger.info("‚úì –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Weaviate")
            logger.info("Weaviate —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å + BM25")
            logger.info("–î–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force")

            # –î–ª—è Weaviate –æ—Ç–¥–µ–ª—å–Ω—ã–π BM25 –Ω–µ –Ω—É–∂–µ–Ω
            return weaviate_indexer, None, chunks_df

        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Weaviate: {e}")
            logger.info("–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Weaviate –∑–∞–ø—É—â–µ–Ω: docker-compose up -d")
            raise

    # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å –Ω—É–ª—è
    logger.info("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤...")

    # === –ü–û–¢–û–ö–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: load ‚Üí clean ‚Üí chunk ‚Üí accumulate/index ===
    # –í–º–µ—Å—Ç–æ batch processing (load all ‚Üí clean all ‚Üí chunk all)
    # –∏—Å–ø–æ–ª—å–∑—É–µ–º streaming (process doc ‚Üí chunk doc ‚Üí accumulate/index)
    from src.streaming_builder import build_knowledge_base_streaming

    logger.info("–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É –∑–∞ —Ä–∞–∑)...")
    logger.info(f"  - LLM –æ—á–∏—Å—Ç–∫–∞: {'–í–ö–õ' if llm_clean else '–í–´–ö–õ'}")
    if llm_clean:
        logger.info(f"  - –ü–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏: {min_usefulness}")
    logger.info(f"  - –†–µ–∂–∏–º: Weaviate (streaming index)")

    with log_timing(logger, "–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
        # –°–æ–∑–¥–∞–µ–º Weaviate indexer
        weaviate_indexer = WeaviateIndexer()

        # –û—á–∏—â–∞–µ–º –µ—Å–ª–∏ force_rebuild
        if force_rebuild:
            logger.info("–û—á–∏—Å—Ç–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ Weaviate...")
            weaviate_indexer.delete_all()

        # –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π –≤ Weaviate
        chunks_df = build_knowledge_base_streaming(
            csv_path=str(WEBSITES_CSV),
            indexer=weaviate_indexer,
            for_weaviate=True,
            llm_clean=llm_clean,
            min_usefulness=min_usefulness,
            chunk_batch_size=500,  # –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ 500 —á–∞–Ω–∫–æ–≤
            csv_chunksize=None     # –∏—Å–ø–æ–ª—å–∑—É–µ–º CSV_CHUNKSIZE –∏–∑ config.py
        )

    logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks_df)}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
    chunks_df.to_pickle(chunks_path)
    logger.info(f"–ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {chunks_path}")

    # 3. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (Weaviate —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –≤ streaming —Ä–µ–∂–∏–º–µ –≤—ã—à–µ)
    weaviate_indexer.chunk_metadata = chunks_df
    logger.info("‚úì Weaviate –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ (streaming mode)!")
    logger.info("–í–∫–ª—é—á–∞–µ—Ç: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å + BM25 (–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫)")
    return weaviate_indexer, None, chunks_df


def process_questions(embedding_indexer, bm25_indexer,
                     questions_df: pd.DataFrame = None) -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ (online —ç—Ç–∞–ø)

    Args:
        embedding_indexer: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å–µ—Ä
        bm25_indexer: BM25 –∏–Ω–¥–µ–∫—Å–µ—Ä
        questions_df: DataFrame —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (–µ—Å–ª–∏ None - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —Ñ–∞–π–ª–∞)

    Returns:
        DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    logger = get_logger(__name__)
    logger.info("="*80)
    logger.info("–≠–¢–ê–ü 2: –û–ë–†–ê–ë–û–¢–ö–ê –í–û–ü–†–û–°–û–í (ONLINE)")
    logger.info("="*80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
    if questions_df is None:
        questions_df = load_and_preprocess_questions(
            str(QUESTIONS_CSV),
            apply_lemmatization=False
        )

    # –°–æ–∑–¥–∞–Ω–∏–µ RAG –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = RAGPipeline(embedding_indexer, bm25_indexer)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
    results = []

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(questions_df)} –≤–æ–ø—Ä–æ—Å–æ–≤...")

    started_at = time.time()
    last_partial_save = time.time()
    save_every = 50  # –∫–∞–∂–¥—ã–µ N –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–π —Ñ–∞–π–ª
    partial_path = OUTPUTS_DIR / "submission_partial.csv"

    for idx, row in tqdm(questions_df.iterrows(), total=len(questions_df)):
        q_id = row['q_id']
        query = row['processed_query']

        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            t0 = time.time()
            result = pipeline.search(query)
            dt = time.time() - t0

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            doc_ids = result['documents_id']

            # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            while len(doc_ids) < 5:
                doc_ids.append(-1)  # –∑–∞–≥–ª—É—à–∫–∞

            results.append({
                'q_id': q_id,
                'web_list': str(doc_ids[:5])
            })

            if (idx + 1) % save_every == 0:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                pd.DataFrame(results).to_csv(partial_path, index=False)
                elapsed = time.time() - started_at
                per_q = elapsed / (idx + 1)
                eta = per_q * (len(questions_df) - (idx + 1))
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {idx + 1}/{len(questions_df)} | {per_q:.2f}s/–≤–æ–ø—Ä–æ—Å | ETA ~ {eta/60:.1f} –º–∏–Ω | —á–∞—Å—Ç–∏—á–Ω—ã–π —Ñ–∞–π–ª: {partial_path}")

            # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –º–µ—Ç—Ä–∏–∫—É
            logger.debug(f"q_id={q_id} | –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤={result.get('num_candidates', 'NA')} | –≤—Ä–µ–º—è={dt:.2f}s | docs={doc_ids[:5]}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ {q_id}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            results.append({
                'q_id': q_id,
                'web_list': '[-1, -1, -1, -1, -1]'
            })

    results_df = pd.DataFrame(results)
    return results_df


def evaluate_on_examples(embedding_indexer, bm25_indexer):
    """
    –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö

    Args:
        embedding_indexer: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å–µ—Ä
        bm25_indexer: BM25 –∏–Ω–¥–µ–∫—Å–µ—Ä

    Returns:
        —Å—Ä–µ–¥–Ω—è—è –º–µ—Ç—Ä–∏–∫–∞
    """
    logger = get_logger(__name__)
    logger.info("="*80)
    logger.info("–û–¶–ï–ù–ö–ê –ù–ê –≠–¢–ê–õ–û–ù–ù–´–• –ü–†–ò–ú–ï–†–ê–•")
    logger.info("="*80)

    from src.config import EXAMPLES_CSV

    examples_df = pd.read_csv(EXAMPLES_CSV)
    pipeline = RAGPipeline(embedding_indexer, bm25_indexer)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ web_id –∏–∑ chunk'–æ–≤
    # (—ç—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏, —É–ø—Ä–æ—Å—Ç–∏–º)

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples_df)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    logger.info("–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ")

    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É recall@5
    # –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å web_id –∏–∑ chunk'–æ–≤ –≤ examples

    return None


def cmd_build(args):
    """–ö–æ–º–∞–Ω–¥–∞: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    logger = get_logger(__name__)
    logger.info("="*80)
    logger.info("–†–ï–ñ–ò–ú: –ü–û–°–¢–†–û–ï–ù–ò–ï –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô")
    logger.info("="*80)

    if args.llm_clean:
        logger.info("[LLM-–†–ï–ñ–ò–ú] –í–∫–ª—é—á–µ–Ω–∞ –æ—á–∏—Å—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM")
        logger.info(f"[LLM-–†–ï–ñ–ò–ú] –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏: {args.min_usefulness}")
        logger.info("[LLM-–†–ï–ñ–ò–ú] –≠—Ç–æ —É–≤–µ–ª–∏—á–∏—Ç –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ 10-20 —Ä–∞–∑!")

    embedding_indexer, bm25_indexer, chunks_df = build_knowledge_base(
        force_rebuild=args.force,
        llm_clean=args.llm_clean,
        min_usefulness=args.min_usefulness
    )

    logger.info("="*80)
    logger.info("[OK] –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –ü–û–°–¢–†–û–ï–ù–ê –£–°–ü–ï–®–ù–û")
    logger.info("="*80)
    logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks_df)}")

    if USE_WEAVIATE and WEAVIATE_AVAILABLE:
        logger.info("–í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å: Weaviate (http://localhost:8080)")
        logger.info("BM25 –∏–Ω–¥–µ–∫—Å: –≤—Å—Ç—Ä–æ–µ–Ω –≤ Weaviate (–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫)")
        try:
            embedding_indexer.close()
        except Exception:
            pass


def cmd_search(args):
    """–ö–æ–º–∞–Ω–¥–∞: –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã"""
    logger = get_logger(__name__)
    logger.info("="*80)
    logger.info("–†–ï–ñ–ò–ú: –û–ë–†–ê–ë–û–¢–ö–ê –í–û–ü–†–û–°–û–í")
    logger.info("="*80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")

    chunks_path = PROCESSED_DIR / "chunks.pkl"

    if not chunks_path.exists():
        logger.error("–û–®–ò–ë–ö–ê: –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: python main_pipeline.py build")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤
    chunks_df = pd.read_pickle(chunks_path)
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks_df)} —á–∞–Ω–∫–æ–≤")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ (Weaviate-only)
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Weaviate (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ + BM25)")
    try:
        embedding_indexer = WeaviateIndexer()
        embedding_indexer.chunk_metadata = chunks_df
        bm25_indexer = None  # –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è Weaviate
        logger.info("‚úì –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Weaviate")
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Weaviate: {e}")
        logger.info("–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Weaviate –∑–∞–ø—É—â–µ–Ω: docker-compose up -d")
        return

    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if args.optimize:
        logger.info("="*80)
        logger.info("GRID SEARCH –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–†–ê–ú–ï–¢–†–û–í")
        logger.info("="*80)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimize_questions_df = load_and_preprocess_questions(
            str(QUESTIONS_CSV),
            apply_lemmatization=False
        )

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π retriever –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        from src.retrieval import HybridRetriever
        temp_retriever = HybridRetriever(embedding_indexer, bm25_indexer)

        # –ó–∞–ø—É—Å–∫–∞–µ–º grid search (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç—ã –∏–∑ config –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ)
        try:
            with log_timing(logger, "Grid Search"):
                best_params = optimize_rag_params(
                    retriever=temp_retriever,
                    questions_df=optimize_questions_df,
                    mode=args.optimize_mode,        # None = –∏–∑ config.GRID_SEARCH_MODE
                    sample_size=args.optimize_sample, # None = –∏–∑ config.GRID_SEARCH_SAMPLE_SIZE
                    use_llm_eval=None               # None = –∏–∑ config.GRID_SEARCH_USE_LLM
                )
            logger.info("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã! –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")

        except Exception as e:
            logger.warning(f"–û–®–ò–ë–ö–ê –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ config.py")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    if args.limit:
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–≤—ã—Ö {args.limit} –≤–æ–ø—Ä–æ—Å–æ–≤ (—Ä–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)")
        questions_df = load_and_preprocess_questions(
            str(QUESTIONS_CSV),
            apply_lemmatization=False
        ).head(args.limit)
    else:
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
        questions_df = None

    with log_timing(logger, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"):
        try:
            results_df = process_questions(embedding_indexer, bm25_indexer, questions_df)
        finally:
            try:
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Weaviate –ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞
                if hasattr(embedding_indexer, 'close'):
                    embedding_indexer.close()
            except Exception:
                pass

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_path = OUTPUTS_DIR / "submission.csv"
    results_df.to_csv(output_path, index=False)

    logger.info("="*80)
    logger.info("[OK] –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    logger.info("="*80)
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_path}")
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(results_df)}")


def cmd_all(args):
    """–ö–æ–º–∞–Ω–¥–∞: –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª (build + search)"""
    logger = get_logger(__name__)
    logger.info("="*80)
    logger.info("–†–ï–ñ–ò–ú: –ü–û–õ–ù–´–ô –¶–ò–ö–õ (BUILD + SEARCH)")
    logger.info("="*80)

    if hasattr(args, 'llm_clean') and args.llm_clean:
        logger.info("[LLM-–†–ï–ñ–ò–ú] –í–∫–ª—é—á–µ–Ω–∞ –æ—á–∏—Å—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM")

    # 1. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    logger.info("[1/2] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
    with log_timing(logger, "–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: build"):
        embedding_indexer, bm25_indexer, chunks_df = build_knowledge_base(
            force_rebuild=args.force,
            llm_clean=getattr(args, 'llm_clean', False),
            min_usefulness=getattr(args, 'min_usefulness', 0.3)
        )

    # 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if getattr(args, 'optimize', False):
        logger.info("="*80)
        logger.info("GRID SEARCH –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–†–ê–ú–ï–¢–†–û–í")
        logger.info("="*80)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimize_questions_df = load_and_preprocess_questions(
            str(QUESTIONS_CSV),
            apply_lemmatization=False
        )

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π retriever –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        from src.retrieval import HybridRetriever
        temp_retriever = HybridRetriever(embedding_indexer, bm25_indexer)

        # –ó–∞–ø—É—Å–∫–∞–µ–º grid search (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç—ã –∏–∑ config –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ)
        try:
            with log_timing(logger, "Grid Search"):
                best_params = optimize_rag_params(
                    retriever=temp_retriever,
                    questions_df=optimize_questions_df,
                    mode=getattr(args, 'optimize_mode', None),        # None = –∏–∑ config.GRID_SEARCH_MODE
                    sample_size=getattr(args, 'optimize_sample', None), # None = –∏–∑ config.GRID_SEARCH_SAMPLE_SIZE
                    use_llm_eval=None               # None = –∏–∑ config.GRID_SEARCH_USE_LLM
                )
            logger.info("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã! –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")

    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    if getattr(args, 'optimize', False):
        logger.info("[3/3] –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤...")
    else:
        logger.info("[2/2] –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤...")

    if args.limit:
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–≤—ã—Ö {args.limit} –≤–æ–ø—Ä–æ—Å–æ–≤ (—Ä–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)")
        questions_df = load_and_preprocess_questions(
            str(QUESTIONS_CSV),
            apply_lemmatization=False
        ).head(args.limit)
    else:
        questions_df = None

    with log_timing(logger, "–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: search"):
        try:
            results_df = process_questions(embedding_indexer, bm25_indexer, questions_df)
        finally:
            try:
                if hasattr(embedding_indexer, 'close'):
                    embedding_indexer.close()
            except Exception:
                pass

    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_path = OUTPUTS_DIR / "submission.csv"
    results_df.to_csv(output_path, index=False)

    logger.info("="*80)
    logger.info("[OK] –ü–û–õ–ù–´–ô –¶–ò–ö–õ –ó–ê–í–ï–†–®–ï–ù")
    logger.info("="*80)
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_path}")
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(results_df)}")


def cmd_check_env(args):
    """–ö–æ–º–∞–Ω–¥–∞: –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    import os
    logger = get_logger(__name__)
    
    logger.info("="*80)
    logger.info("–ü–†–û–í–ï–†–ö–ê –ü–ï–†–ï–ú–ï–ù–ù–´–• –û–ö–†–£–ñ–ï–ù–ò–Ø –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò")
    logger.info("="*80)
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º config –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    from src import config
    
    def check_env_var(name, value, required=False, sensitive=False):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        has_value = bool(value and str(value).strip())
        status = "‚úÖ" if (has_value if required else True) else "‚ùå"
        
        if sensitive and has_value:
            # –ú–∞—Å–∫–∏—Ä—É–µ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ 8 –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–∏–º–≤–æ–ª–∞)
            value_str = str(value)
            if len(value_str) > 12:
                masked = value_str[:8] + "..." + value_str[-4:]
            else:
                masked = "***"
            display_value = masked
        else:
            display_value = value if has_value else "(–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)"
        
        logger.info(f"{status} {name:30s} = {display_value}")
        if required and not has_value:
            logger.warning(f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: {name} –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã!")
    
    logger.info("\nüìã –û–°–ù–û–í–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –û–ö–†–£–ñ–ï–ù–ò–Ø:\n")
    
    # LLM —Ä–µ–∂–∏–º
    logger.info("ü§ñ LLM –ù–ê–°–¢–†–û–ô–ö–ò:")
    check_env_var("LLM_MODE", config.LLM_MODE)
    check_env_var("LLM_API_MODEL", config.LLM_API_MODEL)
    check_env_var("LLM_API_ROUTING", config.LLM_API_ROUTING if config.LLM_API_ROUTING else "(–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)")
    check_env_var("OPENROUTER_API_KEY", config.OPENROUTER_API_KEY, required=(config.LLM_MODE == "API"), sensitive=True)
    logger.info(f"   LLM_API_MAX_WORKERS = {config.LLM_API_MAX_WORKERS}")
    logger.info(f"   LLM_API_TIMEOUT = {config.LLM_API_TIMEOUT}s")
    logger.info(f"   LLM_API_RETRIES = {config.LLM_API_RETRIES}")
    
    # Grid Search
    logger.info("\nüîç GRID SEARCH –ù–ê–°–¢–†–û–ô–ö–ò:")
    logger.info(f"   GRID_SEARCH_MODE = {config.GRID_SEARCH_MODE}")
    logger.info(f"   GRID_SEARCH_SAMPLE_SIZE = {config.GRID_SEARCH_SAMPLE_SIZE}")
    logger.info(f"   GRID_SEARCH_USE_LLM = {config.GRID_SEARCH_USE_LLM}")
    
    # Weaviate
    logger.info("\nüíæ WEAVIATE –ù–ê–°–¢–†–û–ô–ö–ò:")
    logger.info(f"   USE_WEAVIATE = {config.USE_WEAVIATE}")
    logger.info(f"   WEAVIATE_URL = {config.WEAVIATE_URL}")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    logger.info("\n‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–†–ê–ë–û–¢–ö–ò:")
    logger.info(f"   CSV_CHUNKSIZE = {config.CSV_CHUNKSIZE}")
    logger.info(f"   LLM_PARALLEL_WORKERS = {config.LLM_PARALLEL_WORKERS}")
    logger.info(f"   FORCE_CPU = {os.environ.get('FORCE_CPU', 'false')}")
    
    # –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏
    logger.info("\nüéõÔ∏è  –§–£–ù–ö–¶–ò–û–ù–ê–õ–¨–ù–´–ï –§–õ–ê–ì–ò:")
    logger.info(f"   ENABLE_QUERY_EXPANSION = {config.ENABLE_QUERY_EXPANSION}")
    logger.info(f"   ENABLE_RRF = {config.ENABLE_RRF}")
    logger.info(f"   ENABLE_CONTEXT_WINDOW = {config.ENABLE_CONTEXT_WINDOW}")
    logger.info(f"   ENABLE_METADATA_FILTER = {config.ENABLE_METADATA_FILTER}")
    logger.info(f"   ENABLE_USEFULNESS_FILTER = {config.ENABLE_USEFULNESS_FILTER}")
    logger.info(f"   ENABLE_DYNAMIC_TOP_K = {config.ENABLE_DYNAMIC_TOP_K}")
    logger.info(f"   RERANKER_TYPE = {config.RERANKER_TYPE}")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.info("\nüìù –õ–û–ì–ò–†–û–í–ê–ù–ò–ï:")
    logger.info(f"   LOG_LEVEL = {config.LOG_LEVEL}")
    logger.info(f"   LOG_FILE = {config.LOG_FILE}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
    logger.info("\n" + "="*80)
    logger.info("–ü–†–û–í–ï–†–ö–ê –ö–†–ò–¢–ò–ß–ï–°–ö–ò–• –ù–ê–°–¢–†–û–ï–ö:")
    logger.info("="*80)
    
    issues = []
    if config.LLM_MODE == "API" and not config.OPENROUTER_API_KEY:
        issues.append("‚ùå OPENROUTER_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è API —Ä–µ–∂–∏–º–∞)")
    
    if config.USE_WEAVIATE:
        try:
            import weaviate
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º v4 API
            if config.WEAVIATE_URL == "http://localhost:8080":
                client = weaviate.connect_to_local()
            else:
                # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ URL –∏—Å–ø–æ–ª—å–∑—É–µ–º connect_to_custom
                from urllib.parse import urlparse
                parsed = urlparse(config.WEAVIATE_URL)
                client = weaviate.connect_to_custom(
                    http_host=parsed.hostname,
                    http_port=parsed.port or 8080,
                    http_secure=parsed.scheme == "https"
                )
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–π
            client.collections.list_all()
            logger.info("‚úÖ Weaviate –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –æ—Ç–≤–µ—á–∞–µ—Ç")
            client.close()
        except Exception as e:
            issues.append(f"‚ùå Weaviate –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            logger.info("   üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: docker-compose up -d")
    
    if config.LLM_MODE == "local":
        model_path = config.MODELS_DIR / config.LLM_MODEL_FILE
        if model_path.exists():
            logger.info(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è LLM –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {config.LLM_MODEL_FILE}")
        else:
            issues.append(f"‚ùå –õ–æ–∫–∞–ª—å–Ω–∞—è LLM –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            logger.info("   üí° –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: python scripts/download_models.py")
    
    if issues:
        logger.warning("\n‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–ë–õ–ï–ú–´:")
        for issue in issues:
            logger.warning(f"   {issue}")
    else:
        logger.info("\n‚úÖ –í—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –ø–æ—Ä—è–¥–∫–µ!")
    
    logger.info("\n" + "="*80)
    logger.info("üí° –ü–û–î–°–ö–ê–ó–ö–ê: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º:")
    logger.info("   export LLM_MODE=api")
    logger.info("   export OPENROUTER_API_KEY=sk-or-v1-...")
    logger.info("   export LLM_API_MODEL=tngtech/deepseek-r1t2-chimera:free")
    logger.info("="*80)


def cmd_evaluate(args):
    """–ö–æ–º–∞–Ω–¥–∞: –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö"""
    logger = get_logger(__name__)
    logger.info("="*80)
    logger.info("–†–ï–ñ–ò–ú: –û–¶–ï–ù–ö–ê –ù–ê –ü–†–ò–ú–ï–†–ê–•")
    logger.info("="*80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ (Weaviate-only)
    chunks_path = PROCESSED_DIR / "chunks.pkl"
    if not chunks_path.exists():
        logger.error("–û–®–ò–ë–ö–ê: –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: python main_pipeline.py build")
        return

    chunks_df = pd.read_pickle(chunks_path)
    try:
        embedding_indexer = WeaviateIndexer()
        embedding_indexer.chunk_metadata = chunks_df
        bm25_indexer = None
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Weaviate: {e}")
        logger.info("–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Weaviate –∑–∞–ø—É—â–µ–Ω: docker-compose up -d")
        return

    # –û—Ü–µ–Ω–∫–∞
    evaluate_on_examples(embedding_indexer, bm25_indexer)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤"""
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–¥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á—Ç–æ–±—ã –ª–æ–≤–∏—Ç—å —Ä–∞–Ω–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
    setup_logging(level=LOG_LEVEL, log_file=LOG_FILE)
    logger = get_logger(__name__)

    parser = argparse.ArgumentParser(
        description="RAG –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫–∞",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

BUILD (—Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π):
  python main_pipeline.py build                           # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
  python main_pipeline.py build --force                   # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
  python main_pipeline.py build --llm-clean               # –° LLM –æ—á–∏—Å—Ç–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
  python main_pipeline.py build --llm-clean --min-usefulness 0.5  # –° —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π

SEARCH (–ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–æ–≤):
  python main_pipeline.py search                          # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã
  python main_pipeline.py search --limit 10               # –¢–µ—Å—Ç –Ω–∞ 10 –≤–æ–ø—Ä–æ—Å–∞—Ö
  python main_pipeline.py search --optimize               # –° –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (grid search)
  python main_pipeline.py search --optimize --optimize-mode test  # –¢–µ—Å—Ç (5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π)
  python main_pipeline.py search --optimize --optimize-mode quick  # –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (54 –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏)
  python main_pipeline.py search --optimize --optimize-mode full  # –ü–æ–ª–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (1225 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π)

ALL (–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª):
  python main_pipeline.py all                             # Build + Search
  python main_pipeline.py all --llm-clean                 # –° LLM –æ—á–∏—Å—Ç–∫–æ–π
  python main_pipeline.py all --llm-clean --optimize --optimize-mode test  # –° LLM –æ—á–∏—Å—Ç–∫–æ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π (test)
  python main_pipeline.py all --llm-clean --optimize --optimize-mode quick  # –° LLM –æ—á–∏—Å—Ç–∫–æ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π (quick)

EVALUATE:
  python main_pipeline.py evaluate                        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö

CHECK-ENV (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏):
  python main_pipeline.py check-env                      # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è')

    # –ö–æ–º–∞–Ω–¥–∞: build
    parser_build = subparsers.add_parser(
        'build',
        help='–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π (–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)'
    )
    parser_build.add_argument(
        '--force',
        action='store_true',
        help='–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç'
    )
    parser_build.add_argument(
        '--llm-clean',
        action='store_true',
        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–º–µ–¥–ª–µ–Ω–Ω–æ, +–∫–∞—á–µ—Å—Ç–≤–æ)'
    )
    parser_build.add_argument(
        '--min-usefulness',
        type=float,
        default=0.3,
        help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è LLM —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (0.0-1.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.3)'
    )
    parser_build.set_defaults(func=cmd_build)

    # –ö–æ–º–∞–Ω–¥–∞: search
    parser_search = subparsers.add_parser(
        'search',
        help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã (—Ç—Ä–µ–±—É–µ—Ç –≥–æ—Ç–æ–≤—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π)'
    )
    parser_search.add_argument(
        '--limit',
        type=int,
        help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –≤–æ–ø—Ä–æ—Å–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)'
    )
    parser_search.add_argument(
        '--optimize',
        action='store_true',
        help='–ó–∞–ø—É—Å—Ç–∏—Ç—å grid search –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º'
    )
    parser_search.add_argument(
        '--optimize-sample',
        type=int,
        default=None,
        help='–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è grid search (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config.GRID_SEARCH_SAMPLE_SIZE)'
    )
    parser_search.add_argument(
        '--optimize-mode',
        type=str,
        default=None,
        choices=['test', 'quick', 'full'],
        help='–†–µ–∂–∏–º grid search: test (5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π), quick (54 –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏) –∏–ª–∏ full (1225 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π) (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config.GRID_SEARCH_MODE)'
    )
    parser_search.set_defaults(func=cmd_search)

    # –ö–æ–º–∞–Ω–¥–∞: all
    parser_all = subparsers.add_parser(
        'all',
        help='–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã'
    )
    parser_all.add_argument(
        '--force',
        action='store_true',
        help='–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç'
    )
    parser_all.add_argument(
        '--llm-clean',
        action='store_true',
        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–º–µ–¥–ª–µ–Ω–Ω–æ, +–∫–∞—á–µ—Å—Ç–≤–æ)'
    )
    parser_all.add_argument(
        '--min-usefulness',
        type=float,
        default=0.3,
        help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è LLM —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (0.0-1.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.3)'
    )
    parser_all.add_argument(
        '--limit',
        type=int,
        help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –≤–æ–ø—Ä–æ—Å–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)'
    )
    parser_all.add_argument(
        '--optimize',
        action='store_true',
        help='–ó–∞–ø—É—Å—Ç–∏—Ç—å grid search –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º'
    )
    parser_all.add_argument(
        '--optimize-sample',
        type=int,
        default=None,
        help='–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è grid search (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config.GRID_SEARCH_SAMPLE_SIZE)'
    )
    parser_all.add_argument(
        '--optimize-mode',
        type=str,
        default=None,
        choices=['test', 'quick', 'full'],
        help='–†–µ–∂–∏–º grid search: test (5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π), quick (54 –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏) –∏–ª–∏ full (1225 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π) (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config.GRID_SEARCH_MODE)'
    )
    parser_all.set_defaults(func=cmd_all)

    # –ö–æ–º–∞–Ω–¥–∞: evaluate
    parser_eval = subparsers.add_parser(
        'evaluate',
        help='–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö'
    )
    parser_eval.set_defaults(func=cmd_evaluate)

    # –ö–æ–º–∞–Ω–¥–∞: check-env
    parser_check = subparsers.add_parser(
        'check-env',
        help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é'
    )
    parser_check.set_defaults(func=cmd_check_env)

    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # –í—ã–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    logger.info("="*80)
    logger.info("RAG –ü–ê–ô–ü–õ–ê–ô–ù –î–õ–Ø –ü–û–ò–°–ö–ê –†–ï–õ–ï–í–ê–ù–¢–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í –ê–õ–¨–§–ê-–ë–ê–ù–ö–ê")
    logger.info("="*80)

    if USE_WEAVIATE and WEAVIATE_AVAILABLE:
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Weaviate –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    else:
        logger.warning("Weaviate –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω, –Ω–æ –ø—Ä–æ–µ–∫—Ç —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ Weaviate-only.")

    if USE_WEAVIATE and not WEAVIATE_AVAILABLE:
        logger.critical("USE_WEAVIATE=true, –Ω–æ weaviate-client –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")

    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
    args.func(args)

    logger.info("[OK] –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()
