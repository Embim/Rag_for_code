# Рекомендации по моделям OpenRouter

## Стандартные большие модели (не экспериментальные)

### Для генерации ответов (RAG_ANSWER_MODEL)

**Рекомендуемые варианты (по приоритету):**

1. **Meta Llama 3.1 405B FREE** ⭐ (используется по умолчанию)
   - `meta-llama/llama-3.1-405b-instruct:free` 
   - Стандартная модель, не экспериментальная
   - 405 миллиардов параметров - самая большая доступная бесплатная модель
   - Показывает сильные результаты по сравнению с GPT-4o и Claude 3.5 Sonnet
   - 131,072 токенов контекста (128k)
   - Оптимизирована для высококачественных диалогов
   - Ссылка: https://openrouter.ai/meta-llama/llama-3.1-405b-instruct:free
   - ⚠️ Может быть медленнее чем 70B из-за размера, но качество максимальное

2. **Meta Llama 3.3 70B FREE** (быстрее, хорошее качество)
   - `meta-llama/llama-3.3-70b-instruct:free`  
   - Стандартная модель, не экспериментальная
   - Стабильная, хорошо работает с кодом
   - 131,072 токенов контекста
   - Поддержка: English, German, French, Italian, Portuguese, Hindi, Spanish, Thai
   - Ссылка: https://openrouter.ai/meta-llama/llama-3.3-70b-instruct:free
   - Платная версия: `meta-llama/llama-3.3-70b-instruct` (быстрее, без лимитов)

3. **Meta Llama 3.1 70B**
   - `meta-llama/llama-3.1-70b-instruct`
   - Проверенная модель, отличное качество

3. **Google Gemini Pro 1.5**
   - `google/gemini-pro-1.5`
   - Хорошо работает с кодом и контекстом

4. **Anthropic Claude 3.5 Sonnet**
   - `anthropic/claude-3.5-sonnet`
   - Отличное качество, но может быть дороже

5. **Qwen 2.5 72B**
   - `qwen/qwen-2.5-72b-instruct`
   - Хорошая альтернатива, поддерживает много языков

### Для оценки качества (RAG_QUALITY_MODEL)

**Быстрые и эффективные модели:**

1. **Meta Llama 3.3 8B** (текущая, рекомендуется)
   - `meta-llama/llama-3.3-8b-instruct:free`
   - Быстрая, достаточно точная для оценки

2. **Meta Llama 3.1 8B**
   - `meta-llama/llama-3.1-8b-instruct:free`

3. **Qwen 2.5 7B**
   - `qwen/qwen-2.5-7b-instruct:free`

### Для переписывания запросов (RAG_REWRITE_MODEL)

**Малые быстрые модели:**

1. **Meta Llama 3.3 8B** (текущая, рекомендуется)
   - `meta-llama/llama-3.3-8b-instruct:free`
   - Быстрая, хорошо переписывает запросы

2. **Meta Llama 3.1 8B**
   - `meta-llama/llama-3.1-8b-instruct:free`

## Настройка через переменные окружения

Добавьте в `.env` файл:

```bash
# Основная модель для генерации ответов (большая модель)
# По умолчанию используется 405B для МАКСИМАЛЬНОГО КАЧЕСТВА (бесплатно)
RAG_ANSWER_MODEL=meta-llama/llama-3.1-405b-instruct:free

# Для более быстрой работы (70B, все еще отличное качество):
# RAG_ANSWER_MODEL=meta-llama/llama-3.3-70b-instruct:free

# Для платной версии (быстрее, без лимитов):
# RAG_ANSWER_MODEL=meta-llama/llama-3.3-70b-instruct

# Модель для оценки качества (малая быстрая)
RAG_QUALITY_MODEL=meta-llama/llama-3.3-8b-instruct:free

# Модель для переписывания запросов (малая быстрая)
RAG_REWRITE_MODEL=meta-llama/llama-3.3-8b-instruct:free

# Общая модель для агентов (если используется)
# По умолчанию используется 405B для МАКСИМАЛЬНОГО КАЧЕСТВА (бесплатно)
CODE_EXPLORER_MODEL=meta-llama/llama-3.1-405b-instruct:free
```

## Бесплатные варианты

**По умолчанию используется 405B бесплатная версия для максимального качества!** Не нужно ничего настраивать.

Если хотите использовать другие бесплатные модели:

```bash
# Бесплатные варианты (могут иметь ограничения по скорости/количеству запросов)
RAG_ANSWER_MODEL=meta-llama/llama-3.1-405b-instruct:free  # ⭐ Текущая по умолчанию (МАКСИМАЛЬНОЕ КАЧЕСТВО)
# или для более быстрой работы:
RAG_ANSWER_MODEL=meta-llama/llama-3.3-70b-instruct:free  # ⚡ Быстрее, все еще отличное качество
# или
RAG_ANSWER_MODEL=qwen/qwen-2.5-72b-instruct:free
# или другие бесплатные модели
```

**Важно:** 
- Бесплатная версия `meta-llama/llama-3.1-405b-instruct:free` - это полноценная модель 405B, самая большая доступная бесплатная модель!
- Может быть медленнее чем 70B из-за размера, но качество ответов максимальное
- Показывает результаты на уровне GPT-4o и Claude 3.5 Sonnet

## Проверка доступности моделей

Для проверки доступности моделей на OpenRouter:
1. Зайдите на https://openrouter.ai/models
2. Найдите нужную модель
3. Проверьте статус доступности
4. Убедитесь, что модель не помечена как "experimental"

## Примечания

- Модели с `:free` могут иметь ограничения по скорости и количеству запросов
- Большие модели (70B+) дают лучшее качество, но медленнее и дороже
- Малые модели (8B) быстрее и дешевле, подходят для простых задач
- Рекомендуется использовать разные модели для разных задач (качество/скорость баланс)

