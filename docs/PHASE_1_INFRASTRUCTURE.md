# Фаза 1. Подготовка инфраструктуры

## Общее описание фазы

Эта фаза закладывает архитектурный фундамент для расширения системы. Сейчас проект заточен под один конкретный use case — поиск по банковским документам в CSV-формате. Чтобы добавить работу с кодом, нужно сначала сделать архитектуру гибкой и расширяемой. Без этого каждая новая функциональность будет костылём поверх существующего кода.

Результат фазы — система с чёткими абстракциями, где документы и код являются равноправными источниками данных, обрабатываемыми через единый интерфейс, но с разными реализациями под капотом.

---

## TODO 1.1: Абстрактный интерфейс DocumentSource

### Что нужно сделать

Создать базовый абстрактный класс, который определяет контракт для любого источника данных в системе. Все источники — будь то CSV с документами, Git-репозиторий с кодом, или в будущем Confluence/Notion — должны реализовывать этот интерфейс.

### Почему это важно

Сейчас в проекте логика загрузки документов размазана между несколькими файлами и жёстко привязана к CSV-формату. При добавлении кода придётся дублировать много логики или делать условные ветвления по типу источника. Абстракция позволяет добавлять новые источники без изменения существующего кода.

### Технические детали

Интерфейс должен определять методы для загрузки данных, получения метаданных, проверки изменений с момента последней индексации, и итерации по элементам источника. Каждый элемент источника (документ или файл с кодом) должен иметь уникальный идентификатор, контент, и набор метаданных.

Для документов метаданные включают URL источника, дату, тему. Для кода — путь к файлу, язык программирования, имя репозитория, хэш коммита. Структура метаданных должна быть расширяемой через словарь, но с типизированными обязательными полями.

Текущая реализация загрузки из CSV (файл preprocessing.py и работа с websites.csv) должна быть обёрнута в класс CSVDocumentSource, реализующий новый интерфейс. Это первый шаг рефакторинга — существующая функциональность продолжает работать, но через новую абстракцию.

### Файлы для изменения

Создать новый файл src/core/sources.py с абстрактным классом. Создать src/sources/csv_source.py с реализацией для CSV. Модифицировать main_pipeline.py для использования новой абстракции вместо прямого вызова функций preprocessing.

### Критерии готовности

Существующий pipeline build работает без изменений в поведении. Добавление нового источника данных требует только создания нового класса, реализующего интерфейс, без изменения core-логики.

---

## TODO 1.2: Абстрактный класс Chunker

### Что нужно сделать

Выделить логику разбиения контента на части (чанкинг) в отдельную абстракцию. Разные типы контента требуют разных стратегий чанкинга, и это должно быть инкапсулировано.

### Почему это важно

Текстовые документы разбиваются по количеству токенов с перекрытием — это работает для прозы. Код нужно разбивать по структурным единицам: функциям, классам, методам. Разбивать функцию пополам бессмысленно, потому что теряется семантическая целостность. Кроме того, для кода важно сохранять контекст — импорты, сигнатуру класса, в котором находится метод.

### Технические детали

Абстрактный класс Chunker определяет методы chunk() для разбиения контента на части и merge_context() для объединения чанков с их контекстом при формировании ответа.

TextChunker (текущая реализация из chunking.py) разбивает по токенам с параметрами CHUNK_SIZE и CHUNK_OVERLAP. Он становится реализацией абстракции.

CodeChunker будет создан в следующих фазах. Его логика: файл разбивается по AST-структуре на функции и классы, каждая функция — отдельный чанк, класс может быть одним большим чанком или разбит на методы в зависимости от размера. Контекст чанка включает импорты файла и сигнатуру родительского класса.

Метод merge_context() важен для code retrieval. Когда система находит релевантный метод, нужно показать его вместе с контекстом — в каком классе он находится, какие импорты использует. Это отличается от документов, где достаточно показать сам чанк с соседями.

### Параметризация

Chunker должен принимать конфигурацию через параметры, а не глобальные константы. Это позволит иметь разные настройки для разных языков программирования. Python-код может чанкироваться с одними параметрами, TypeScript — с другими.

### Файлы для изменения

Создать src/core/chunkers.py с абстрактным классом. Рефакторить src/chunking.py в src/chunkers/text_chunker.py. Обновить indexing.py для использования абстракции.

---

## TODO 1.3: Профили конфигурации

### Что нужно сделать

Реорганизовать систему конфигурации так, чтобы можно было переключаться между разными режимами работы. Режим "documents" для текущей функциональности, режим "code" для работы с репозиториями, режим "hybrid" для совместного поиска.

### Почему это важно

Сейчас конфигурация в config.py и .env содержит параметры специфичные для поиска по документам: размер чанка 200 токенов, определённые веса для гибридного поиска, конкретная embedding-модель. Для кода оптимальные параметры будут другими. Нужна возможность иметь несколько наборов параметров и переключаться между ними.

### Технические детали

Предлагается структура с базовой конфигурацией и профилями-оверрайдами. Базовая конфигурация содержит общие параметры: URL Weaviate, настройки LLM API, параметры логирования. Профили переопределяют специфичные параметры.

Профиль "documents" содержит текущие настройки: CHUNK_SIZE=200, CHUNK_OVERLAP=50, EMBEDDING_MODEL="BAAI/bge-m3", веса гибридного поиска.

Профиль "code" будет содержать: другую стратегию чанкинга (по AST), возможно другую embedding-модель (специализированную для кода), другие веса поиска (больший вес BM25, потому что для кода лексический поиск важнее).

Профиль "hybrid" комбинирует оба, с возможностью указать пропорции при поиске.

### Реализация

Использовать Pydantic Settings для типизированной конфигурации с валидацией. Профили хранятся в YAML-файлах в директории config/profiles/. При запуске указывается профиль через --profile аргумент или переменную окружения.

Структура директории config/:
- base.yaml — общие настройки
- profiles/documents.yaml
- profiles/code.yaml  
- profiles/hybrid.yaml

### Файлы для изменения

Создать директорию config/ с файлами профилей. Рефакторить src/config.py для загрузки профилей. Обновить main_pipeline.py для поддержки --profile аргумента.

---

## TODO 1.4: Система плагинов для парсеров

### Что нужно сделать

Создать архитектуру плагинов, где каждый парсер языка программирования или фреймворка регистрирует себя в системе и обрабатывает файлы с определёнными расширениями.

### Почему это важно

Поддержка разных языков и фреймворков — это основная точка расширения системы. Сегодня нужен Python и TypeScript, завтра может понадобиться Go или Java. Без архитектуры плагинов добавление нового языка потребует изменений в core-коде. С плагинами достаточно создать новый модуль и зарегистрировать его.

### Технические детали

Плагин парсера определяется интерфейсом с методами: get_supported_extensions() возвращает список расширений файлов (.py, .ts, .tsx); parse_file() принимает путь к файлу и возвращает структурированное представление кода; get_entities() возвращает список сущностей (функции, классы, компоненты) извлечённых из файла.

Регистрация плагинов происходит через декоратор или явный вызов register_parser(). При загрузке модуля парсер автоматически добавляется в реестр. Система использует реестр для выбора подходящего парсера по расширению файла.

Для сложных случаев (один файл требует нескольких парсеров, например .py файл с Django-моделями) поддерживается цепочка парсеров. Базовый Python-парсер извлекает структуру, Django-парсер дополняет информацией о моделях и вьюхах.

### Структура плагина

Каждый плагин — отдельный модуль в директории src/code_rag/parsers/. Файл __init__.py этой директории автоматически импортирует все плагины при загрузке, что вызывает их регистрацию.

Плагин должен уметь работать с ошибками парсинга. Синтаксически некорректный файл не должен ломать индексацию всего репозитория. Плагин логирует ошибку и пропускает файл или извлекает частичную информацию.

### Расширяемость

Архитектура должна позволять пользователям добавлять свои плагины без модификации основного кода. Плагины из директории plugins/ в рабочей директории пользователя должны автоматически подгружаться.

### Файлы для создания

src/core/parser_registry.py — реестр и базовый интерфейс парсера. src/code_rag/parsers/__init__.py — автозагрузка плагинов. Примеры плагинов будут созданы в фазе 2.

---

## TODO 1.5: Расширение Weaviate-схемы для кода

### Что нужно сделать

Добавить в Weaviate новые коллекции для хранения кодовых сущностей с соответствующими полями и связями между объектами.

### Почему это важно

Текущая схема оптимизирована для документов — есть коллекция с текстовыми чанками и их метаданными. Код требует более сложной структуры: разные типы сущностей (файлы, функции, классы, эндпоинты), связи между ними (вызывает, импортирует, наследует), принадлежность к репозиторию.

### Новые коллекции

Repository — информация о репозитории: имя, URL, тип (frontend/backend), язык, фреймворк, дата последней индексации, хэш коммита.

CodeFile — файл с кодом: путь, язык, размер, хэш содержимого, ссылка на Repository.

Function — функция или метод: имя, сигнатура, docstring, тело (код), номера строк начала и конца, ссылка на CodeFile, ссылка на родительский Class (если метод).

Class — класс: имя, docstring, список базовых классов, ссылка на CodeFile.

Component — React-компонент: имя, props с типами, используемые хуки, ссылка на CodeFile.

Endpoint — API-эндпоинт: путь, HTTP-метод, параметры, модель запроса, модель ответа, ссылка на Function-обработчик.

Model — модель данных (Django Model, Pydantic Model): имя, поля с типами, связи с другими моделями.

### Cross-references

Weaviate поддерживает ссылки между объектами разных коллекций. Использовать это для связей:
- Function → Function (calls)
- Function → Class (belongs_to)
- Component → Endpoint (sends_request_to)
- Endpoint → Function (handled_by)
- Endpoint → Model (request_body, response_body)
- Model → Model (foreign_key, many_to_many)

### Vectorization

Каждая сущность должна иметь vector embedding для семантического поиска. Embedding строится не только из кода сущности, но и из контекста: имя файла, имена связанных сущностей, docstring. Это улучшает качество поиска.

### Миграция

Существующая схема для документов остаётся без изменений. Новые коллекции добавляются параллельно. Это позволяет сохранить обратную совместимость и использовать оба типа данных одновременно.

### Файлы для изменения

Создать src/code_rag/schema.py с определением новых коллекций. Обновить indexing.py для создания схемы при инициализации. Добавить миграции схемы для обновления существующих инсталляций.

---

## TODO 1.6: Версионирование индекса кода

### Что нужно сделать

Реализовать механизм отслеживания изменений в репозитории и инкрементального обновления индекса, чтобы не переиндексировать весь репозиторий при каждом изменении.

### Почему это важно

Репозитории меняются часто — несколько коммитов в день это норма. Полная переиндексация даже среднего репозитория занимает минуты. Это неприемлемо для production-использования, где индекс должен быть актуальным.

### Технические детали

При первой индексации сохраняется хэш коммита (HEAD) репозитория. При последующих запросах на индексацию сравнивается текущий HEAD с сохранённым.

Если HEAD изменился, через git diff получается список изменённых файлов между двумя коммитами. Только эти файлы переиндексируются. Добавленные файлы индексируются, удалённые удаляются из индекса, изменённые переиндексируются.

Для изменённых файлов важно обновить не только сам файл, но и связи. Если функция была переименована или удалена, ссылки на неё из других сущностей должны быть обновлены или помечены как broken.

### Обработка переименований

Git отслеживает переименования файлов. Система должна использовать эту информацию, чтобы не удалять и создавать заново сущности при переименовании, а обновлять путь.

### Хранение версий

В коллекции Repository хранится current_commit_hash. История индексаций хранится отдельно: какой коммит, когда проиндексирован, сколько файлов изменено. Это полезно для отладки и аудита.

### Параллельная индексация

При инкрементальном обновлении важно не блокировать поиск. Обновление происходит атомарно: сначала индексируются все изменения во временные объекты, затем атомарный swap заменяет старые объекты новыми.

### Файлы для создания

src/code_rag/version_tracker.py — отслеживание версий и вычисление diff. Интеграция в существующий indexing.py.

---

## Рекомендуемые технологии для фазы

### Pydantic v2

Использовать для всех моделей данных и конфигурации. Pydantic обеспечивает типизацию, валидацию, сериализацию. Pydantic Settings удобен для работы с переменными окружения и файлами конфигурации.

### ABC (Abstract Base Classes)

Стандартный модуль Python для определения абстрактных классов. Простой и не требует дополнительных зависимостей.

### GitPython

Библиотека для работы с Git-репозиториями из Python. Позволяет клонировать репозитории, получать diff между коммитами, читать историю.

### YAML для конфигурации

PyYAML или ruamel.yaml для чтения файлов конфигурации. YAML читабельнее JSON для конфигурационных файлов и поддерживает комментарии.

---

## Оценка трудозатрат

TODO 1.1 (DocumentSource): 2-3 дня — требует аккуратного рефакторинга существующего кода.

TODO 1.2 (Chunker): 1-2 дня — логика уже есть, нужно выделить абстракцию.

TODO 1.3 (Профили): 2-3 дня — новая система конфигурации, миграция существующих настроек.

TODO 1.4 (Плагины): 2-3 дня — архитектура реестра, без реализации конкретных парсеров.

TODO 1.5 (Weaviate схема): 2-3 дня — проектирование схемы, написание миграций.

TODO 1.6 (Версионирование): 3-4 дня — сложная логика с edge cases.

Общая оценка фазы: 2-3 недели.

---

## Критерии завершения фазы

Существующая функциональность работы с документами не сломана и проходит все существующие тесты. Добавление нового источника данных (например, JSON вместо CSV) требует только создания нового класса без изменения core-кода. Система конфигурации поддерживает профили и переключение между ними. Weaviate-схема расширена и готова к приёму кодовых сущностей. Написаны unit-тесты для новых абстракций.
