"""
Grid Search –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ RAG

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ main_pipeline.py —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ --optimize

–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
- TOP_K_DENSE: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
- TOP_K_BM25: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ BM25
- TOP_K_RERANK: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ reranking
- HYBRID_ALPHA: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É dense –∏ BM25

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –æ—Ü–µ–Ω–∫—É:
- –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (semantic similarity)
- LLM as Judge (Context Relevance, Precision, Sufficiency)
"""
import sys
from pathlib import Path
import pandas as pd
from itertools import product
from tqdm import tqdm
from typing import Dict, Tuple
import src.config as config
from src.config import GRID_SEARCH_USE_LLM
from src.logger import get_logger, log_timing


class GridSearchOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ RAG —á–µ—Ä–µ–∑ grid search

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –æ—Ü–µ–Ω–∫—É (cosine + LLM metrics)
    """

    def __init__(self, retriever, questions_df: pd.DataFrame, use_llm_eval: bool = None):
        """
        Args:
            retriever: HybridRetriever –∏–ª–∏ WeaviateIndexer
            questions_df: DataFrame —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            use_llm_eval: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (None = –∏–∑ config.GRID_SEARCH_USE_LLM)
        """
        self.retriever = retriever
        self.questions_df = questions_df

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ config –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–æ —è–≤–Ω–æ
        if use_llm_eval is None:
            use_llm_eval = GRID_SEARCH_USE_LLM

        self.use_llm_eval = use_llm_eval

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Hybrid Evaluator
        self.evaluator = None
        if use_llm_eval:
            try:
                from src.llm_evaluator import get_hybrid_evaluator
                from src.config import LLM_MODE
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã (API –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π)
                use_api = (LLM_MODE == "api")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger = get_logger(__name__)
                logger.info(f"[GridSearch] LLM_MODE –∏–∑ config: {LLM_MODE}")
                logger.info(f"[GridSearch] use_api –±—É–¥–µ—Ç: {use_api}")
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º singleton –µ—Å–ª–∏ —Ä–µ–∂–∏–º –∏–∑–º–µ–Ω–∏–ª—Å—è
                # (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω —Ä–∞–Ω–µ–µ —Å –¥—Ä—É–≥–∏–º —Ä–µ–∂–∏–º–æ–º)
                import src.llm_evaluator as llm_eval_module
                if hasattr(llm_eval_module, '_evaluator_instance') and llm_eval_module._evaluator_instance is not None:
                    existing_use_api = llm_eval_module._evaluator_instance.use_api
                    if existing_use_api != use_api:
                        logger.info(f"[GridSearch] –°–±—Ä–∞—Å—ã–≤–∞–µ–º singleton evaluator (–±—ã–ª {existing_use_api}, –Ω—É–∂–µ–Ω {use_api})")
                        llm_eval_module._evaluator_instance = None
                
                self.evaluator = get_hybrid_evaluator(
                    use_llm=True,
                    semantic_weight=0.3,  # 30% –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                    llm_weight=0.7,       # 70% LLM –º–µ—Ç—Ä–∏–∫–∏
                    use_api=use_api
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ evaluator –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤ –Ω—É–∂–Ω–æ–º —Ä–µ–∂–∏–º–µ
                actual_mode = "API" if self.evaluator.use_api else "–ª–æ–∫–∞–ª—å–Ω—ã–π"
                mode_str = "API" if use_api else "–ª–æ–∫–∞–ª—å–Ω—ã–π"
                logger.info(f"[GridSearch] ‚úì Hybrid Evaluator –∑–∞–≥—Ä—É–∂–µ–Ω (–∑–∞–ø—Ä–æ—à–µ–Ω: {mode_str}, —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π: {actual_mode}, cosine 30% + LLM 70%)")
                
                if self.evaluator.use_api != use_api:
                    logger.error(f"[GridSearch] ‚ùå –û–®–ò–ë–ö–ê: evaluator –≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ! –ó–∞–ø—Ä–æ—à–µ–Ω: {use_api}, —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π: {self.evaluator.use_api}")
            except Exception as e:
                get_logger(__name__).warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LLM Evaluator: {e}")
                get_logger(__name__).warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ")
                self.use_llm_eval = False

    def define_param_grid(self, mode: str = "quick") -> dict:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Args:
            mode: "quick" (–±—ã—Å—Ç—Ä—ã–π) –∏–ª–∏ "full" (–ø–æ–ª–Ω—ã–π)

        Returns:
            dict —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        if mode == "quick":
            # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫
            param_grid = {
                "TOP_K_DENSE": [15, 25, 35],
                "TOP_K_BM25": [15, 25, 35],
                "TOP_K_RERANK": [15, 20],
                "HYBRID_ALPHA": [0.4, 0.5, 0.6]
            }
        else:
            # –ü–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫
            param_grid = {
                "TOP_K_DENSE": [10, 15, 20, 25, 30, 35, 40],
                "TOP_K_BM25": [10, 15, 20, 25, 30, 35, 40],
                "TOP_K_RERANK": [10, 15, 20, 25, 30],
                "HYBRID_ALPHA": [0.3, 0.4, 0.5, 0.6, 0.7]
            }

        total_combinations = (
            len(param_grid["TOP_K_DENSE"]) *
            len(param_grid["TOP_K_BM25"]) *
            len(param_grid["TOP_K_RERANK"]) *
            len(param_grid["HYBRID_ALPHA"])
        )

        logger = get_logger(__name__)
        logger.info(f"üìä Grid Search —Ä–µ–∂–∏–º: {mode}")
        logger.info(f"–í—Å–µ–≥–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {total_combinations}")

        return param_grid

    def evaluate_params(self, params: Dict) -> Tuple[float, Dict]:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Args:
            params: —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Returns:
            (avg_score, detailed_metrics)
            - avg_score: –∏—Ç–æ–≥–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ (hybrid_score –µ—Å–ª–∏ LLM –≤–∫–ª—é—á–µ–Ω, –∏–Ω–∞—á–µ semantic)
            - detailed_metrics: –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        original_params = {
            "TOP_K_DENSE": config.TOP_K_DENSE,
            "TOP_K_BM25": config.TOP_K_BM25,
            "TOP_K_RERANK": config.TOP_K_RERANK,
            "HYBRID_ALPHA": config.HYBRID_ALPHA,
        }

        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            config.TOP_K_DENSE = params["TOP_K_DENSE"]
            config.TOP_K_BM25 = params["TOP_K_BM25"]
            config.TOP_K_RERANK = params["TOP_K_RERANK"]
            config.HYBRID_ALPHA = params["HYBRID_ALPHA"]

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            all_queries = []
            all_results = []

            for idx, row in self.questions_df.iterrows():
                query = row.get('processed_query', row.get('question', ''))

                try:
                    # –ü–æ–∏—Å–∫
                    results = self.retriever.search(query)

                    if len(results) > 0:
                        all_queries.append(query)
                        all_results.append(results)

                except Exception as e:
                    # –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å
                    pass

            # –û—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ Hybrid Evaluator
            if self.use_llm_eval and self.evaluator:
                # LLM-based –æ—Ü–µ–Ω–∫–∞ (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Ç–æ—á–Ω–æ)
                metrics = self.evaluator.evaluate_batch(
                    all_queries,
                    all_results,
                    top_k=params["TOP_K_RERANK"]
                )

                avg_score = metrics['avg_hybrid_score']
                detailed_metrics = metrics

            else:
                # Fallback: —Ç–æ–ª—å–∫–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–±—ã—Å—Ç—Ä–æ)
                total_score = 0.0
                for results in all_results:
                    if len(results) > 0:
                        top_scores = results.head(5)['final_score'].tolist()
                        total_score += sum(top_scores) / len(top_scores)

                avg_score = total_score / len(all_results) if len(all_results) > 0 else 0.0
                detailed_metrics = {
                    'avg_semantic_score': avg_score,
                    'num_evaluated': len(all_results)
                }

            return avg_score, detailed_metrics

        finally:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            config.TOP_K_DENSE = original_params["TOP_K_DENSE"]
            config.TOP_K_BM25 = original_params["TOP_K_BM25"]
            config.TOP_K_RERANK = original_params["TOP_K_RERANK"]
            config.HYBRID_ALPHA = original_params["HYBRID_ALPHA"]

    def search(self, param_grid: dict) -> Tuple[Dict, pd.DataFrame]:
        """
        –ó–∞–ø—É—Å–∫ grid search

        Args:
            param_grid: —Å–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Returns:
            (best_params, results_df)
        """
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        keys = list(param_grid.keys())
        combinations = list(product(*[param_grid[k] for k in keys]))

        logger = get_logger(__name__)
        logger.info("üîç –ó–∞–ø—É—Å–∫ Grid Search...")
        logger.info(f"–ö–æ–º–±–∏–Ω–∞—Ü–∏–π: {len(combinations)} | –í–æ–ø—Ä–æ—Å–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ: {len(self.questions_df)}")

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        best_score = -1
        best_params = None

        for combo in tqdm(combinations, desc="Grid Search"):
            params = dict(zip(keys, combo))

            # –û—Ü–µ–Ω–∫–∞
            logger.debug(f"–û—Ü–µ–Ω–∫–∞: {params}")
            score, detailed_metrics = self.evaluate_params(params)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            result = {
                **params,
                "avg_score": score,
                **{f"metric_{k}": v for k, v in detailed_metrics.items() if k != 'num_evaluated'}
            }
            results.append(result)

            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if self.use_llm_eval and self.evaluator:
                logger.info(
                    f"  Dense={params['TOP_K_DENSE']}, BM25={params['TOP_K_BM25']}, "
                    f"Rerank={params['TOP_K_RERANK']}, Œ±={params['HYBRID_ALPHA']:.1f} ‚Üí "
                    f"Hybrid={score:.3f} "
                    f"(semantic={detailed_metrics.get('avg_semantic_score', 0):.3f}, "
                    f"sufficiency={detailed_metrics.get('avg_context_sufficiency', 0):.3f})"
                )
            else:
                logger.info(
                    f"  Dense={params['TOP_K_DENSE']}, BM25={params['TOP_K_BM25']}, "
                    f"Rerank={params['TOP_K_RERANK']}, Œ±={params['HYBRID_ALPHA']:.1f} ‚Üí "
                    f"Score={score:.3f}"
                )

            # –û–±–Ω–æ–≤–ª—è–µ–º best
            if score > best_score:
                best_score = score
                best_params = params.copy()

        results_df = pd.DataFrame(results).sort_values(by="avg_score", ascending=False)

        return best_params, results_df

    def apply_best_params(self, best_params: Dict):
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫ config

        Args:
            best_params: —Å–ª–æ–≤–∞—Ä—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        logger = get_logger(__name__)
        logger.info("‚≠ê –õ–£–ß–®–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        logger.info(f"TOP_K_DENSE={best_params['TOP_K_DENSE']}, TOP_K_BM25={best_params['TOP_K_BM25']}, TOP_K_RERANK={best_params['TOP_K_RERANK']}, HYBRID_ALPHA={best_params['HYBRID_ALPHA']:.2f}")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ config
        config.TOP_K_DENSE = best_params['TOP_K_DENSE']
        config.TOP_K_BM25 = best_params['TOP_K_BM25']
        config.TOP_K_RERANK = best_params['TOP_K_RERANK']
        config.HYBRID_ALPHA = best_params['HYBRID_ALPHA']

        logger.info("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –∫ config")


def optimize_rag_params(retriever, questions_df: pd.DataFrame,
                       mode: str = None,
                       sample_size: int = None,
                       use_llm_eval: bool = None) -> Dict:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ RAG –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

    Args:
        retriever: HybridRetriever –∏–ª–∏ WeaviateIndexer
        questions_df: DataFrame —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏
        mode: "quick" –∏–ª–∏ "full" (None = –∏–∑ config.GRID_SEARCH_MODE)
        sample_size: —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (None = –∏–∑ config.GRID_SEARCH_SAMPLE_SIZE)
        use_llm_eval: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (None = –∏–∑ config.GRID_SEARCH_USE_LLM)

    Returns:
        best_params: —Å–ª–æ–≤–∞—Ä—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    logger = get_logger(__name__)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ config –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã —è–≤–Ω–æ
    if mode is None:
        from src.config import GRID_SEARCH_MODE
        mode = GRID_SEARCH_MODE

    if sample_size is None:
        from src.config import GRID_SEARCH_SAMPLE_SIZE
        sample_size = GRID_SEARCH_SAMPLE_SIZE

    if use_llm_eval is None:
        use_llm_eval = GRID_SEARCH_USE_LLM

    # –í—ã–±–æ—Ä–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    if len(questions_df) > sample_size:
        sample_df = questions_df.sample(n=sample_size, random_state=42)
    else:
        sample_df = questions_df

    logger.info("="*80)
    logger.info("GRID SEARCH –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø RAG –ü–ê–†–ê–ú–ï–¢–†–û–í")
    logger.info("="*80)
    logger.info(f"–†–µ–∂–∏–º: {mode}")
    logger.info(f"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {sample_size} –≤–æ–ø—Ä–æ—Å–æ–≤")
    logger.info(f"–†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏: {'LLM + Cosine (–≥–∏–±—Ä–∏–¥–Ω—ã–π)' if use_llm_eval else '–¢–æ–ª—å–∫–æ Cosine'}")

    # –°–æ–∑–¥–∞–µ–º optimizer
    optimizer = GridSearchOptimizer(retriever, sample_df, use_llm_eval=use_llm_eval)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_grid = optimizer.define_param_grid(mode=mode)

    # –ó–∞–ø—É—Å–∫–∞–µ–º grid search
    best_params, results_df = optimizer.search(param_grid)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("üìä –¢–æ–ø-5 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π:")
    logger.info("\n" + results_df.head(5).to_string())

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    optimizer.apply_best_params(best_params)

    return best_params


if __name__ == "__main__":
    print("Grid Search Optimizer")
    print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–µ—Ä–µ–∑ main_pipeline.py search --optimize")
