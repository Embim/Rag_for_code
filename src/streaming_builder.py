"""
–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

–í–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–º—è—Ç—å (batch processing):
    documents_df (–≤—Å–µ) ‚Üí llm_clean (–≤—Å–µ) ‚Üí chunk (–≤—Å–µ) ‚Üí index (–≤—Å–µ)

–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (streaming):
    –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: load ‚Üí clean ‚Üí chunk ‚Üí –Ω–∞–∫–æ–ø–∏—Ç—å ‚Üí index –±–∞—Ç—á

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ (–Ω–µ –¥–µ—Ä–∂–∏–º –≤–µ—Å—å DataFrame)
- –ë—ã—Å—Ç—Ä–µ–µ (–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ –º–µ—Ä–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è Weaviate)
- –ü—Ä–æ–≥—Ä–µ—Å—Å –≤–∏–¥–µ–Ω —Å—Ä–∞–∑—É
"""
import pandas as pd
import json
from pathlib import Path
from typing import List, Dict, Optional, Union
import gc
from tqdm import tqdm

from src.preprocessing import TextPreprocessor
from src.chunking import DocumentChunker
from src.logger import get_logger, log_timing
from src.config import CSV_CHUNKSIZE, CSV_COUNT_CHUNKSIZE

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


class StreamingDocumentProcessor:
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: load ‚Üí clean ‚Üí chunk ‚Üí accumulate

    –£–º–µ–Ω—å—à–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É
    –≤–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –ø–∞–º—è—Ç—å.
    """

    def __init__(self,
                 llm_clean: bool = False,
                 min_usefulness: float = 0.3,
                 chunk_batch_size: int = 500,
                 csv_chunksize: int = None):
        """
        Args:
            llm_clean: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            min_usefulness: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (0.0-1.0)
            chunk_batch_size: —Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π –±–∞—Ç—á–∞
            csv_chunksize: —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ CSV —á–∏—Ç–∞—Ç—å –∑–∞ —Ä–∞–∑ (–µ—Å–ª–∏ None - –∏–∑ config.CSV_CHUNKSIZE)
        """
        self.llm_clean = llm_clean
        self.min_usefulness = min_usefulness
        self.chunk_batch_size = chunk_batch_size
        self.csv_chunksize = csv_chunksize if csv_chunksize is not None else CSV_CHUNKSIZE

        self.logger = get_logger(__name__)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.preprocessor = TextPreprocessor()
        self.chunker = DocumentChunker()

        # LLM cleaner (–∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
        self.llm_cleaner = None
        self.llm_clean_failed = False  # —Ñ–ª–∞–≥ —á—Ç–æ LLM –æ—á–∏—Å—Ç–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
        if llm_clean:
            try:
                from src.llm_preprocessing import LLMDocumentCleaner
                self.llm_cleaner = LLMDocumentCleaner(verbose=True)
                self.llm_cleaner.load_model()
                self.logger.info("‚úì LLM Document Cleaner –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception as e:
                self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LLM cleaner: {e}")
                self.logger.warning("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ LLM –æ—á–∏—Å—Ç–∫–∏")
                self.llm_clean_failed = True
                # –ù–ï –º–µ–Ω—è–µ–º self.llm_clean –Ω–∞ False, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ

    def process_document(self, doc_row: pd.Series) -> List[Dict]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: preprocess ‚Üí llm_clean ‚Üí chunk

        Args:
            doc_row: —Å—Ç—Ä–æ–∫–∞ –∏–∑ CSV (pandas Series)

        Returns:
            list of chunk dicts
        """
        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_text = self.preprocessor.preprocess_document(
            text=doc_row.get('text', ''),
            title=doc_row.get('title', ''),
            apply_lemmatization=False
        )

        if not processed_text or len(processed_text.strip()) < 10:
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            return []

        # 2. LLM –æ—á–∏—Å—Ç–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
        entities = ''
        topics = ''

        if self.llm_clean and self.llm_cleaner:
            try:
                cleaned_result = self.llm_cleaner.clean_document(processed_text)

                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏
                usefulness = cleaned_result.get('usefulness_score', 1.0)
                is_useful = cleaned_result.get('is_useful', True)

                if is_useful and usefulness >= self.min_usefulness:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                    processed_text = cleaned_result.get('clean_text', processed_text)

                    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    products = cleaned_result.get('products', [])
                    actions = cleaned_result.get('actions', [])
                    conditions = cleaned_result.get('conditions', [])

                    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º entities
                    all_entities = products + actions + conditions
                    if all_entities:
                        entities = json.dumps(all_entities, ensure_ascii=False)

                    # –¢–µ–º—ã
                    topics_list = cleaned_result.get('topics', [])
                    if topics_list:
                        topics = json.dumps(topics_list, ensure_ascii=False)
                else:
                    # –î–æ–∫—É–º–µ–Ω—Ç –±–µ—Å–ø–æ–ª–µ–∑–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    self.logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç web_id={doc_row.get('web_id')} (usefulness={usefulness:.2f})")
                    return []

            except Exception as e:
                # –û—à–∏–±–∫–∞ LLM - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
                self.logger.debug(f"–û—à–∏–±–∫–∞ LLM –¥–ª—è web_id={doc_row.get('web_id')}: {e}")
                pass

        # 3. –ß–∞–Ω–∫–∏–Ω–≥
        chunks = self.chunker.chunk_by_words(
            text=processed_text,
            web_id=int(doc_row.get('web_id', 0)),
            title=str(doc_row.get('title', '')),
            url=str(doc_row.get('url', '')),
            kind=str(doc_row.get('kind', '')),
            entities=entities,
            topics=topics
        )

        return chunks

    def process_csv_streaming(self,
                             csv_path: str,
                             indexer = None,
                             for_weaviate: bool = False) -> Optional[pd.DataFrame]:
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CSV (—Ä–µ–∂–∏–º Weaviate):
        - –ß–∏—Ç–∞–µ—Ç –ø–æ csv_chunksize –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ —Ä–∞–∑
        - –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π: preprocess ‚Üí llm_clean ‚Üí chunk
        - –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç —á–∞–Ω–∫–∏ –≤ –±–∞—Ç—á–∏
        - –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –±–∞—Ç—á–∏ —Å—Ä–∞–∑—É –≤ Weaviate –∏ –æ—á–∏—â–∞–µ—Ç –ø–∞–º—è—Ç—å

        Args:
            csv_path: –ø—É—Ç—å –∫ websites.csv
            indexer: WeaviateIndexer (–¥–ª—è streaming –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏) –∏–ª–∏ None
            for_weaviate: True –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Weaviate (–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Å—Ä–∞–∑—É)

        Returns:
            DataFrame —Å–æ –≤—Å–µ–º–∏ —á–∞–Ω–∫–∞–º–∏ (–¥–ª—è FAISS) –∏–ª–∏ None (–¥–ª—è Weaviate)
        """
        self.logger.info("="*80)
        self.logger.info("–ü–û–¢–û–ö–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í")
        self.logger.info("="*80)
        self.logger.info(f"–†–µ–∂–∏–º: Weaviate (streaming index)")
        self.logger.info(f"LLM –æ—á–∏—Å—Ç–∫–∞: {'–í–ö–õ' if self.llm_clean and not self.llm_clean_failed else '–í–´–ö–õ'}")
        if self.llm_clean:
            if self.llm_clean_failed:
                # LLM –æ—á–∏—Å—Ç–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
                from src.config import MODELS_DIR, LLM_MODEL_FILE
                model_path = MODELS_DIR / LLM_MODEL_FILE
                self.logger.warning(f"‚ö† LLM –æ—á–∏—Å—Ç–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                self.logger.warning(f"   –û–∂–∏–¥–∞–µ–º—ã–π –ø—É—Ç—å: {model_path}")
                self.logger.warning(f"   –õ–æ–≥–∏ –Ω–µ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å—Å—è. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: python scripts/download_models.py")
                print(f"  ‚ö† LLM –æ—á–∏—Å—Ç–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                print(f"     –û–∂–∏–¥–∞–µ–º—ã–π –ø—É—Ç—å: {model_path}")
                print(f"     –õ–æ–≥–∏ –Ω–µ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å—Å—è. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: python scripts/download_models.py")
            elif self.llm_cleaner:
                self.logger.info(f"–ü–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏: {self.min_usefulness}")
                self.logger.info(f"‚úì LLM Document Cleaner –≥–æ—Ç–æ–≤ (–ª–æ–≥–∏: outputs/llm_cleaning.log)")
                print(f"  ‚úì LLM Document Cleaner –≥–æ—Ç–æ–≤ (–ª–æ–≥–∏: outputs/llm_cleaning.log)")
            else:
                self.logger.warning("‚ö† LLM Document Cleaner –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω - –ª–æ–≥–∏ –Ω–µ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å—Å—è!")
                print(f"  ‚ö† LLM Document Cleaner –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω - –ª–æ–≥–∏ –Ω–µ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å—Å—è!")
        else:
            self.logger.info("‚Ñπ LLM –æ—á–∏—Å—Ç–∫–∞ –≤—ã–∫–ª—é—á–µ–Ω–∞ - —Ñ–∞–π–ª llm_cleaning.log –±—É–¥–µ—Ç –ø—É—Å—Ç—ã–º")
            print(f"  ‚Ñπ LLM –æ—á–∏—Å—Ç–∫–∞ –í–´–ö–õ–Æ–ß–ï–ù–ê (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å --llm-clean –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)")
        self.logger.info(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —á–∞–Ω–∫–æ–≤: {self.chunk_batch_size}")
        self.logger.info(f"–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ CSV: {self.csv_chunksize} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        self.logger.info("="*80)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        print("\n" + "="*80)
        print("üìä –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        print("="*80)
        try:
            # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–¥—Å—á–µ—Ç: —á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É –±–æ–ª—å—à–∏–º–∏ –±–∞—Ç—á–∞–º–∏
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–æ–π chunksize (–∏–∑ config) —Ç.–∫. –º—ã —Ç–æ–ª—å–∫–æ —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏,
            # –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ - —ç—Ç–æ –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ —á–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–≥–¥–µ chunksize=5-10)
            total_docs = 0
            for chunk in pd.read_csv(csv_path, chunksize=CSV_COUNT_CHUNKSIZE, usecols=[0]):
                total_docs += len(chunk)
            print(f"‚úì –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ CSV: {total_docs:,}")
        except Exception as e:
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã: {e}")
            print("   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä")
            total_docs = None
        print("="*80 + "\n")

        all_chunks = []
        chunk_batch = []

        total_docs_processed = 0
        total_docs_filtered = 0
        total_chunks_created = 0
        batches_indexed = 0

        # –ß–∏—Ç–∞–µ–º CSV –ø–æ —á–∞—Å—Ç—è–º (streaming)
        csv_reader = pd.read_csv(csv_path, chunksize=self.csv_chunksize)

        # –°–æ–∑–¥–∞–µ–º tqdm –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        desc = "üöÄ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"
        if self.llm_clean:
            desc += " (—Å LLM –æ—á–∏—Å—Ç–∫–æ–π)"
        pbar = tqdm(
            total=total_docs,
            desc=desc,
            unit="–¥–æ–∫",
            ncols=100,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )

        try:
            for csv_chunk_idx, doc_chunk_df in enumerate(csv_reader):
                self.logger.info(f"\n[–ë–∞—Ç—á CSV {csv_chunk_idx + 1}] –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(doc_chunk_df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

                for idx, doc_row in doc_chunk_df.iterrows():
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    doc_chunks = self.process_document(doc_row)

                    if doc_chunks:
                        chunk_batch.extend(doc_chunks)
                        total_chunks_created += len(doc_chunks)
                    else:
                        total_docs_filtered += 1

                    total_docs_processed += 1
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                    pbar.update(1)
                    pbar.set_postfix({
                        '—á–∞–Ω–∫–æ–≤': total_chunks_created,
                        '–æ—Ç—Ñ–∏–ª—å—Ç—Ä.': total_docs_filtered,
                        '–±–∞—Ç—á–µ–π': batches_indexed
                    })

                    # –ï—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –±–∞—Ç—á–∞
                    if len(chunk_batch) >= self.chunk_batch_size:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
                        batch_df = pd.DataFrame(chunk_batch)

                        if for_weaviate and indexer is not None:
                            # Weaviate: –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Å—Ä–∞–∑—É
                            self.logger.info(f"  ‚Üí –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞—Ç—á–∞ {batches_indexed + 1}: {len(batch_df)} —á–∞–Ω–∫–æ–≤ –≤ Weaviate...")

                            with log_timing(self.logger, f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞—Ç—á–∞ {batches_indexed + 1}"):
                                indexer.index_documents(batch_df, show_progress=False)

                            batches_indexed += 1

                            # –î–ª—è Weaviate: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤ (–±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
                            all_chunks.extend(chunk_batch)

                            # –û—á–∏—â–∞–µ–º –±–∞—Ç—á –∏–∑ –ø–∞–º—è—Ç–∏
                            chunk_batch = []
                            del batch_df

                            # –ß–∏—Å—Ç–∏–º GPU –ø–∞–º—è—Ç—å
                            gc.collect()
                            if TORCH_AVAILABLE and torch.cuda.is_available():
                                torch.cuda.empty_cache()
                        else:
                            raise RuntimeError("–û–∂–∏–¥–∞–ª—Å—è —Ä–µ–∂–∏–º Weaviate —Å –∞–∫—Ç–∏–≤–Ω—ã–º indexer")

            # –ü—Ä–æ–≥—Ä–µ—Å—Å —É–∂–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ tqdm
            self.logger.info(
                f"  –ü—Ä–æ–≥—Ä–µ—Å—Å: {total_docs_processed} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | "
                f"–ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {total_chunks_created} | "
                f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_docs_filtered}"
            )
        
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            pbar.close()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–∞
        if chunk_batch:
            batch_df = pd.DataFrame(chunk_batch)

            if for_weaviate and indexer is not None:
                self.logger.info(f"  ‚Üí –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –±–∞—Ç—á–∞: {len(batch_df)} —á–∞–Ω–∫–æ–≤...")
                indexer.index_documents(batch_df, show_progress=False)
                batches_indexed += 1

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                all_chunks.extend(chunk_batch)
                del batch_df
            else:
                raise RuntimeError("–û–∂–∏–¥–∞–ª—Å—è —Ä–µ–∂–∏–º Weaviate —Å –∞–∫—Ç–∏–≤–Ω—ã–º indexer")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "="*80)
        print("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        print("="*80)
        print(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_docs_processed:,}")
        print(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_docs_filtered:,} ({total_docs_filtered/max(total_docs_processed,1)*100:.1f}%)")
        print(f"–ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {total_chunks_created:,}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤/–¥–æ–∫—É–º–µ–Ω—Ç: {total_chunks_created/max(total_docs_processed-total_docs_filtered,1):.1f}")

        if for_weaviate:
            print(f"–ë–∞—Ç—á–µ–π –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ Weaviate: {batches_indexed}")

        print("="*80 + "\n")
        
        # –î—É–±–ª–∏—Ä—É–µ–º –≤ –ª–æ–≥
        self.logger.info("\n" + "="*80)
        self.logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        self.logger.info("="*80)
        self.logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_docs_processed}")
        self.logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_docs_filtered} ({total_docs_filtered/max(total_docs_processed,1)*100:.1f}%)")
        self.logger.info(f"–ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {total_chunks_created}")
        self.logger.info(f"–°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤/–¥–æ–∫—É–º–µ–Ω—Ç: {total_chunks_created/max(total_docs_processed-total_docs_filtered,1):.1f}")

        if for_weaviate:
            self.logger.info(f"–ë–∞—Ç—á–µ–π –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ Weaviate: {batches_indexed}")

        self.logger.info("="*80)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
        if all_chunks:
            chunks_df = pd.DataFrame(all_chunks)
            self.logger.info(f"DataFrame —Å–æ–∑–¥–∞–Ω: {len(chunks_df)} —Å—Ç—Ä–æ–∫")
            return chunks_df
        else:
            self.logger.warning("–ù–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω–æ!")
            return pd.DataFrame()


def build_knowledge_base_streaming(csv_path: str,
                                   indexer = None,
                                   for_weaviate: bool = False,
                                   llm_clean: bool = False,
                                   min_usefulness: float = 0.3,
                                   chunk_batch_size: int = 500,
                                   csv_chunksize: int = None) -> Optional[pd.DataFrame]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø–æ—Ç–æ–∫–æ–≤—ã–º –º–µ—Ç–æ–¥–æ–º

    Args:
        csv_path: –ø—É—Ç—å –∫ websites.csv
        indexer: WeaviateIndexer –∏–ª–∏ None
        for_weaviate: True –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Weaviate
        llm_clean: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –æ—á–∏—Å—Ç–∫—É
        min_usefulness: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏
        chunk_batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        csv_chunksize: —Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–∏—Ç–∞—Ç—å –∑–∞ —Ä–∞–∑ (–µ—Å–ª–∏ None - –∏–∑ config.CSV_CHUNKSIZE)

    Returns:
        DataFrame —Å —á–∞–Ω–∫–∞–º–∏ (–¥–ª—è FAISS) –∏–ª–∏ None (–¥–ª—è Weaviate)
    """
    processor = StreamingDocumentProcessor(
        llm_clean=llm_clean,
        min_usefulness=min_usefulness,
        chunk_batch_size=chunk_batch_size,
        csv_chunksize=csv_chunksize
    )

    chunks_df = processor.process_csv_streaming(
        csv_path=csv_path,
        indexer=indexer,
        for_weaviate=for_weaviate
    )

    return chunks_df


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
    print("StreamingDocumentProcessor")
    print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–µ—Ä–µ–∑ main_pipeline.py build")
