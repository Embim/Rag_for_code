"""
LLM-based –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ main_pipeline.py —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ --llm-clean

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:
1. –£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä (–Ω–∞–≤–∏–≥–∞—Ü–∏—è, —Ñ—É—Ç–µ—Ä—ã, —Ä–µ–∫–ª–∞–º–∞) —á–µ—Ä–µ–∑ LLM
2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, —Ç–µ—Ä–º–∏–Ω—ã)
3. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
4. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å
5. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
"""
import pandas as pd
from llama_cpp import Llama
from tqdm import tqdm
import json
import re
from pathlib import Path
from typing import Dict, Optional
import logging
from logging.handlers import RotatingFileHandler

from src.config import (
    LLM_MODEL_FILE,
    LLM_CONTEXT_SIZE,
    LLM_GPU_LAYERS,
    LLM_MAX_TOKENS,
    LLM_N_BATCH,
    LLM_N_THREADS,
    MODELS_DIR,
    OUTPUTS_DIR,
)


class LLMDocumentCleaner:
    """
    LLM-based –æ—á–∏—Å—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen3-32B (–∏–ª–∏ –¥—Ä—É–≥—É—é LLM) –¥–ª—è:
    - –£–¥–∞–ª–µ–Ω–∏—è –º—É—Å–æ—Ä–∞ –∏–∑ –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –î–æ–±–∞–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    """

    def __init__(self, model_path: Optional[str] = None, verbose: bool = True):
        """
        Args:
            model_path: –ø—É—Ç—å –∫ GGUF –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑ config)
            verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        """
        if model_path is None:
            model_path = str(MODELS_DIR / LLM_MODEL_FILE)

        self.model_path = model_path
        self.verbose = verbose
        self.llm = None

        # –û—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–±–æ—Ç—ã LLM
        # (—á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–µ—Ä–Ω—É–ª–∞ –º–æ–¥–µ–ª—å)
        self.llm_logger = logging.getLogger("llm_cleaning")
        self._init_llm_logger()

        if verbose:
            print(f"\n{'='*80}")
            print(f"üì• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Document Cleaner")
            print(f"   –ú–æ–¥–µ–ª—å: {Path(model_path).name}")
            print(f"{'='*80}\n")

    def _init_llm_logger(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ª–æ–≥-—Ñ–∞–π–ª–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM –æ—á–∏—Å—Ç–∫–∏.

        –§–æ—Ä–º–∞—Ç: –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º clean_document.
        """
        try:
            OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
        except Exception:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é ‚Äî —Ç–∏—Ö–æ –≤—ã—Ö–æ–¥–∏–º, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω
            return

        log_path = OUTPUTS_DIR / "llm_cleaning.log"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –ª–∏ —É–∂–µ —Ö–µ–Ω–¥–ª–µ—Ä –Ω–∞ —ç—Ç–æ—Ç —Ñ–∞–π–ª
        handler_exists = any(
            isinstance(h, RotatingFileHandler) and getattr(h, "baseFilename", None) == str(log_path)
            for h in self.llm_logger.handlers
        )
        if handler_exists:
            return

        handler = RotatingFileHandler(
            str(log_path),
            maxBytes=25 * 1024 * 1024,
            backupCount=3,
            encoding="utf-8",
        )
        # –õ–æ–≥–∏—Ä—É–µ–º –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–µ (JSON)
        formatter = logging.Formatter("%(message)s")
        handler.setFormatter(formatter)

        # INFO –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Ç.–∫. –∫–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å ‚Äî –æ–¥–∏–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç LLM
        handler.setLevel(logging.INFO)
        self.llm_logger.setLevel(logging.INFO)
        self.llm_logger.addHandler(handler)
        # –ù–µ –¥—É–±–ª–∏—Ä—É–µ–º –≤ root, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        self.llm_logger.propagate = False
        
        # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ (—Ç–æ–ª—å–∫–æ –≤ verbose —Ä–µ–∂–∏–º–µ)
        if self.verbose:
            print(f"  üìù LLM –ª–æ–≥-—Ñ–∞–π–ª: {log_path}")
            print(f"     –•–µ–Ω–¥–ª–µ—Ä–æ–≤: {len(self.llm_logger.handlers)}")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ LLM –º–æ–¥–µ–ª–∏"""
        if self.llm is not None:
            return  # —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

        model_path = Path(self.model_path)

        if not model_path.exists():
            raise FileNotFoundError(
                f"LLM –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\n"
                f"–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: python download_models.py"
            )

        if self.verbose:
            print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")

        self.llm = Llama(
            model_path=str(model_path),
            n_ctx=LLM_CONTEXT_SIZE,
            n_gpu_layers=LLM_GPU_LAYERS,
            n_batch=LLM_N_BATCH,  # –∏–∑ config (1024 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
            n_threads=LLM_N_THREADS,  # –∏–∑ config (16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
            use_mlock=True,
            verbose=False
        )

        if self.verbose:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def clean_document(self, text: str) -> Dict:
        """
        –û—á–∏—Å—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM

        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            dict —Å –ø–æ–ª—è–º–∏:
                - clean_text: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                - products: —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                - actions: —Å–ø–∏—Å–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π
                - conditions: —Å–ø–∏—Å–æ–∫ —É—Å–ª–æ–≤–∏–π
                - topics: —Å–ø–∏—Å–æ–∫ —Ç–µ–º
                - usefulness_score: –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (0-1)
                - is_useful: bool
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        text_truncated = text[:3000]  # –±—ã–ª–æ 4000, —É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è

        # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (—É–±—Ä–∞–Ω—ã –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è)
        prompt = f"""–û—á–∏—Å—Ç–∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–µ—Ä–Ω–∏ JSON:

–î–û–ö–£–ú–ï–ù–¢:
{text_truncated}

–ó–ê–î–ê–ß–ò:
1. –£–¥–∞–ª–∏: –Ω–∞–≤–∏–≥–∞—Ü–∏—é, —Ñ—É—Ç–µ—Ä—ã, —Ä–µ–∫–ª–∞–º—É, cookie-–±–∞–Ω–Ω–µ—Ä—ã, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
2. –°–æ—Ö—Ä–∞–Ω–∏: –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —á–∏—Å–ª–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, —Å—Ä–æ–∫–∏)
3. –¢–µ–º—ã (–º–∞–∫—Å 3): –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ_–∫–∞—Ä—Ç—ã, –¥–µ–±–µ—Ç–æ–≤—ã–µ_–∫–∞—Ä—Ç—ã, –ø–µ—Ä–µ–≤–æ–¥—ã, –∂–∫—Ö, –∫—ç—à–±—ç–∫, —Å—á–µ—Ç–∞_—Ä–µ–∫–≤–∏–∑–∏—Ç—ã, –∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –º–æ–±–∏–ª—å–Ω–æ–µ_–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –∞–ª—å—Ñ–∞_–æ–Ω–ª–∞–π–Ω, –∏–ø–æ—Ç–µ–∫–∞, –∫—Ä–µ–¥–∏—Ç—ã, –≤–∫–ª–∞–¥—ã, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ
4. –ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: 0.0-0.3 (–º—É—Å–æ—Ä), 0.4-0.6 (—á–∞—Å—Ç–∏—á–Ω–æ), 0.7-1.0 (–∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞)

JSON:
{{
  "clean_text": "–æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
  "topics": ["—Ç–µ–º–∞_1", "—Ç–µ–º–∞_2"],
  "usefulness_score": 0.0
}}"""

        try:
            response = self.llm(
                prompt,
                max_tokens=LLM_MAX_TOKENS,  # –∏–∑ config.py (1024 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
                temperature=0.1,
                stop=["<|im_end|>"],
                top_p=0.9,  # nucleus sampling –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                top_k=40,  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            )

            response_text = response['choices'][0]['text']

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
            # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π:
            raw_result = None
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –∏—â–µ–º –ø–µ—Ä–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç, –Ω–∞—á–∏–Ω–∞—è —Å –ø–µ—Ä–≤–æ–π {
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∂–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–≥–æ JSON –æ–±—ä–µ–∫—Ç–∞
            first_brace = response_text.find('{')
            if first_brace != -1:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ–∑–∏—Ü–∏–∏ –ø–µ—Ä–≤–æ–π {
                # –ò–¥–µ–º –æ—Ç –∫–æ–Ω—Ü–∞ –∫ –Ω–∞—á–∞–ª—É, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
                brace_count = 0
                last_brace = -1
                for i in range(first_brace, len(response_text)):
                    if response_text[i] == '{':
                        brace_count += 1
                    elif response_text[i] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_brace = i
                            break
                
                if last_brace != -1:
                    try:
                        json_str = response_text[first_brace:last_brace + 1]
                        raw_result = json.loads(json_str)
                    except json.JSONDecodeError:
                        pass
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –º–µ–∂–¥—É –ø–µ—Ä–≤—ã–º–∏ { –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ }
            if raw_result is None:
                first_brace = response_text.find('{')
                last_brace = response_text.rfind('}')
                if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                    try:
                        json_str = response_text[first_brace:last_brace + 1]
                        raw_result = json.loads(json_str)
                    except json.JSONDecodeError:
                        pass
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —Ç–∞–º —Ç–æ–ª—å–∫–æ JSON)
            if raw_result is None:
                try:
                    raw_result = json.loads(response_text.strip())
                except json.JSONDecodeError:
                    pass
            
            if raw_result:
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–µ–π –ø–æ–¥ downstream:
                # –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏
                raw_result.setdefault("clean_text", text_truncated)
                raw_result.setdefault("topics", [])
                raw_result.setdefault("usefulness_score", 0.5)
                # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: —ç—Ç–∏ –ø–æ–ª—è –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è downstream (entities —Å–±–æ—Ä–∫–∞)
                raw_result.setdefault("products", [])
                raw_result.setdefault("actions", [])
                raw_result.setdefault("conditions", [])
                # derive is_useful –ø–æ –ø—Ä–µ–∂–Ω–µ–π –ª–æ–≥–∏–∫–µ (–ø–æ—Ä–æ–≥ ~0.3)
                raw_result["is_useful"] = bool(raw_result.get("usefulness_score", 0.5) >= 0.3)

                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥-—Ñ–∞–π–ª (–±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —à—É–º–∞)
                self._log_llm_result(raw_result, original_text=text_truncated)

                return raw_result
            else:
                # Fallback –µ—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
                fallback = self._fallback_result(text_truncated)
                self._log_llm_result(fallback, original_text=text_truncated, reason="json_parse_failed")
                return fallback

        except Exception as e:
            if self.verbose:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            fallback = self._fallback_result(text_truncated)
            self._log_llm_result(fallback, original_text=text_truncated, reason=str(e))
            return fallback

    def _fallback_result(self, text: str) -> Dict:
        """Fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ LLM –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        return {
            "clean_text": text,
            "products": [],
            "actions": [],
            "conditions": [],
            "topics": [],
            "usefulness_score": 0.5,
            "is_useful": True
        }

    def _log_llm_result(self, result: Dict, original_text: str, reason: Optional[str] = None) -> None:
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ LLM –æ—á–∏—Å—Ç–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON-–ª–æ–≥.

        –ú—ã –ª–æ–≥–∏—Ä—É–µ–º:
        - –∫—Ä–∞—Ç–∫–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ,
        - —É—Å–µ—á—ë–Ω–Ω—ã–π original_text –∏ clean_text (—á—Ç–æ–±—ã –ª–æ–≥ –Ω–µ —Ä–∞–∑—Ä–∞—Å—Ç–∞–ª—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ).
        """
        if not self.llm_logger.handlers:
            # –õ–æ–≥–≥–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª)
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            try:
                self._init_llm_logger()
            except Exception:
                pass
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Ö–µ–Ω–¥–ª–µ—Ä–æ–≤ - –≤—ã—Ö–æ–¥–∏–º
            if not self.llm_logger.handlers:
                return

        try:
            log_record = {
                "reason": reason,
                "usefulness_score": result.get("usefulness_score"),
                "is_useful": result.get("is_useful"),
                "topics": result.get("topics", []),
                "products": result.get("products", []),
                "actions": result.get("actions", []),
                "conditions": result.get("conditions", []),
                # web_id –º–æ–∂–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–∞—Ö ‚Äî –∑–¥–µ—Å—å –æ–±—ã—á–Ω–æ None,
                # –Ω–æ –ø–æ–ª–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è –µ—Å–ª–∏ –≤ –±—É–¥—É—â–µ–º —Ç—É–¥–∞ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å
                "web_id": result.get("web_id"),
                # –ü—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–æ–≤ (—É—Å–µ–∫–∞–µ–º, —á—Ç–æ–±—ã —Ñ–∞–π–ª –±—ã–ª —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
                "original_text_preview": original_text[:1000],
                "clean_text_preview": str(result.get("clean_text", ""))[:1000],
            }
            self.llm_logger.info(json.dumps(log_record, ensure_ascii=False))
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä—ã –≤—Å–µ—Ö —Ö–µ–Ω–¥–ª–µ—Ä–æ–≤
            for handler in self.llm_logger.handlers:
                handler.flush()
        except Exception as e:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –ª–æ–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
            # –ù–æ –º–æ–∂–µ–º –≤—ã–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –≤ verbose —Ä–µ–∂–∏–º–µ
            if self.verbose:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
            pass

    def process_documents(self, documents_df: pd.DataFrame,
                         text_column: str = 'text') -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏:
                - clean_text
                - products (JSON list)
                - actions (JSON list)
                - conditions (JSON list)
                - topics (JSON list)
                - usefulness_score (float)
                - is_useful (bool)
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        results = []

        if self.verbose:
            print(f"\nüöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents_df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM...")
            iterator = tqdm(documents_df.iterrows(), total=len(documents_df), desc="LLM Cleaning")
        else:
            iterator = documents_df.iterrows()

        for idx, row in iterator:
            text = row[text_column]

            # –û—á–∏—â–∞–µ–º —á–µ—Ä–µ–∑ LLM
            cleaned = self.clean_document(text)

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ + –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            result_row = {
                **row.to_dict(),
                'clean_text': cleaned.get('clean_text', text),
                'products': json.dumps(cleaned.get('products', []), ensure_ascii=False),
                'actions': json.dumps(cleaned.get('actions', []), ensure_ascii=False),
                'conditions': json.dumps(cleaned.get('conditions', []), ensure_ascii=False),
                'topics': json.dumps(cleaned.get('topics', []), ensure_ascii=False),
                'usefulness_score': cleaned.get('usefulness_score', 0.5),
                'is_useful': cleaned.get('is_useful', True)
            }

            results.append(result_row)

        result_df = pd.DataFrame(results)

        if self.verbose:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            useful_count = result_df['is_useful'].sum()
            avg_score = result_df['usefulness_score'].mean()

            print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
            print(f"   –ü–æ–ª–µ–∑–Ω—ã—Ö: {useful_count}/{len(result_df)} ({useful_count/len(result_df)*100:.1f}%)")
            print(f"   –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.2f}")

            # –¢–æ–ø —Ç–µ–º—ã
            all_topics = []
            for topics_json in result_df['topics']:
                try:
                    topics = json.loads(topics_json)
                    all_topics.extend(topics)
                except:
                    pass

            if all_topics:
                from collections import Counter
                topic_counts = Counter(all_topics)

                print(f"\nüìä –¢–æ–ø-5 —Ç–µ–º:")
                for topic, count in topic_counts.most_common(5):
                    print(f"   {topic}: {count}")

        return result_df

    def filter_by_usefulness(self, documents_df: pd.DataFrame,
                            min_score: float = 0.3) -> pd.DataFrame:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–ø–æ—Å–ª–µ process_documents)
            min_score: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ usefulness_score

        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        before_count = len(documents_df)

        filtered_df = documents_df[
            (documents_df['is_useful'] == True) &
            (documents_df['usefulness_score'] >= min_score)
        ].copy()

        after_count = len(filtered_df)
        removed_count = before_count - after_count

        if self.verbose:
            print(f"\nüóëÔ∏è  –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (min_score={min_score}):")
            print(f"   –ë—ã–ª–æ: {before_count}")
            print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {after_count}")
            print(f"   –£–¥–∞–ª–µ–Ω–æ: {removed_count} ({removed_count/before_count*100:.1f}%)")

        return filtered_df


def apply_llm_cleaning(documents_df: pd.DataFrame,
                      min_usefulness: float = 0.3,
                      verbose: bool = True) -> pd.DataFrame:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM –æ—á–∏—Å—Ç–∫–∏

    Args:
        documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        min_usefulness: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π DataFrame
    """
    cleaner = LLMDocumentCleaner(verbose=verbose)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    cleaned_df = cleaner.process_documents(documents_df)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if min_usefulness > 0:
        cleaned_df = cleaner.filter_by_usefulness(cleaned_df, min_score=min_usefulness)

    return cleaned_df


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
    print("="*80)
    print("–¢–ï–°–¢ LLM DOCUMENT CLEANER")
    print("="*80)

    # –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    test_doc = """
    –ì–ª–∞–≤–Ω–∞—è / –ö–∞—Ä—Ç—ã / –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞

    –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞ - –¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å –∫—ç—à–±—ç–∫–æ–º

    –ü–æ–ª—É—á–∞–π—Ç–µ –∫—ç—à–±—ç–∫ 2% –Ω–∞ –≤—Å–µ –ø–æ–∫—É–ø–∫–∏ –∏ –¥–æ 10% –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –Ω–∞ –≤—ã–±–æ—Ä.
    –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–∏ —Å—É–º–º–µ –ø–æ–∫—É–ø–æ–∫ –æ—Ç 10000 —Ä—É–±–ª–µ–π –≤ –º–µ—Å—è—Ü.

    –û—Ñ–æ—Ä–º–∏—Ç—å –æ–Ω–ª–∞–π–Ω

    ¬© 2001-2025 –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫. –õ–∏—Ü–µ–Ω–∑–∏—è –¶–ë –†–§ ‚Ññ1326
    """

    test_df = pd.DataFrame([{'text': test_doc, 'web_id': 1}])

    cleaner = LLMDocumentCleaner(verbose=True)
    result_df = cleaner.process_documents(test_df)

    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢:")
    print("="*80)
    print(f"\n–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{result_df.iloc[0]['clean_text']}")
    print(f"\n–ü—Ä–æ–¥—É–∫—Ç—ã: {result_df.iloc[0]['products']}")
    print(f"–¢–µ–º—ã: {result_df.iloc[0]['topics']}")
    print(f"–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: {result_df.iloc[0]['usefulness_score']}")
