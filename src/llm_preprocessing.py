"""
LLM-based –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ main_pipeline.py —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ --llm-clean

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:
1. –£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä (–Ω–∞–≤–∏–≥–∞—Ü–∏—è, —Ñ—É—Ç–µ—Ä—ã, —Ä–µ–∫–ª–∞–º–∞) —á–µ—Ä–µ–∑ LLM
2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, —Ç–µ—Ä–º–∏–Ω—ã)
3. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
4. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å
5. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
"""
import pandas as pd
from llama_cpp import Llama
from tqdm import tqdm
import json
import re
import hashlib
from pathlib import Path
from typing import Dict, Optional
import logging
from logging.handlers import RotatingFileHandler
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from src.config import (
    LLM_MODEL_FILE,
    LLM_CONTEXT_SIZE,
    LLM_GPU_LAYERS,
    LLM_MAX_TOKENS,
    LLM_N_BATCH,
    LLM_N_THREADS,
    LLM_MODE,
    LLM_API_MODEL,
    LLM_API_MAX_WORKERS,
    LLM_API_TIMEOUT,
    LLM_API_RETRIES,
    LLM_API_ROUTING,
    LLM_PARALLEL_WORKERS,
    OPENROUTER_API_KEY,
    MODELS_DIR,
    OUTPUTS_DIR,
)


class LLMDocumentCleanerAPI:
    """
    API-based –æ—á–∏—Å—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ OpenRouter
    
    OpenRouter –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π API –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ 400+ –º–æ–¥–µ–ª—è–º:
    - OpenAI (GPT-4, GPT-3.5)
    - Anthropic (Claude)
    - Google (Gemini)
    - Meta (Llama)
    - DeepSeek (R1T2 Chimera - –±–µ—Å–ø–ª–∞—Ç–Ω–æ!)
    - –ò –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (—É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 10-20 —Ä–∞–∑)
    - –ù–µ –∑–∞–Ω–∏–º–∞–µ—Ç VRAM
    - –ë—ã—Å—Ç—Ä–µ–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    - –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã (DeepSeek R1T2 Chimera)
    
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: DeepSeek R1T2 Chimera (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –±—ã—Å—Ç—Ä–∞—è, —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    """
    
    def __init__(self, verbose: bool = True):
        """
        Args:
            verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        """
        self.verbose = verbose
        self.model = LLM_API_MODEL
        self.max_workers = LLM_API_MAX_WORKERS
        self.timeout = LLM_API_TIMEOUT
        self.retries = LLM_API_RETRIES
        
        # –ö—ç—à –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self._cache = {}
        self._cache_max_size = 100
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è API –∫–ª–∏–µ–Ω—Ç–∞
        self.client = None
        self._init_api_client()
        
        # –õ–æ–≥–≥–µ—Ä
        self.llm_logger = logging.getLogger("llm_cleaning")
        self._init_llm_logger()
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"üì° –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Document Cleaner (OpenRouter API)")
            print(f"   –ú–æ–¥–µ–ª—å: {self.model}")
            print(f"   –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.max_workers}")
            print(f"{'='*80}\n")
    
    def _init_api_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenRouter API –∫–ª–∏–µ–Ω—Ç–∞"""
        # OpenRouter –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
        # OpenRouter —Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á –¥–∞–∂–µ –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        try:
            from openai import OpenAI
            # OpenRouter endpoint
            base_url = "https://openrouter.ai/api/v1"
            
            # OpenRouter —Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á –¥–∞–∂–µ –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            if not OPENROUTER_API_KEY:
                raise ValueError(
                    "OPENROUTER_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!\n"
                    "–ü–æ–ª—É—á–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á –Ω–∞ https://openrouter.ai/keys\n"
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export OPENROUTER_API_KEY=sk-or-v1-..."
                )
            
            # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è OpenRouter
            default_headers = {
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                "HTTP-Referer": "https://github.com/your-repo",  # –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å
                "X-Title": "AlfaBank RAG Pipeline"
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –¥–ª—è —Ä–æ—É—Ç–∏–Ω–≥–∞ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
            if LLM_API_ROUTING:
                default_headers["X-OpenRouter-Provider"] = LLM_API_ROUTING
            
            self.client = OpenAI(
                base_url=base_url,
                api_key=OPENROUTER_API_KEY,
                timeout=self.timeout,
                default_headers=default_headers
            )
        except ImportError:
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai: pip install openai")
    
    def _init_llm_logger(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.llm_logger.handlers:
            log_file = OUTPUTS_DIR / "llm_cleaning.log"
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            handler = RotatingFileHandler(
                log_file,
                maxBytes=10 * 1024 * 1024,  # 10 MB
                backupCount=5,
                encoding='utf-8'
            )
            handler.setFormatter(logging.Formatter(
                '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            ))
            self.llm_logger.addHandler(handler)
            self.llm_logger.setLevel(logging.INFO)
    
    def _extract_final_answer(self, text: str) -> str:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏–∑ reasoning –º–æ–¥–µ–ª–µ–π
        
        Reasoning –º–æ–¥–µ–ª–∏ (sherlock-think-alpha, deepseek-r1 –∏ —Ç.–¥.) –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç
        reasoning –ø—Ä–æ—Ü–µ—Å—Å –≤ —Ç–µ–≥–∞—Ö <think>, <think> –∏ —Ç.–¥.
        –ù—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç.
        """
        if not text or not isinstance(text, str):
            return text
        
        original_text = text
        
        # –£–¥–∞–ª—è–µ–º reasoning —Ç–µ–≥–∏ –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (—Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã)
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<reasoning>.*?</reasoning>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<think>.*', '', text, flags=re.DOTALL | re.IGNORECASE)  # –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–π —Ç–µ–≥
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å reasoning-–º–∞—Ä–∫–µ—Ä–æ–≤
        lines = text.split('\n')
        cleaned_lines = []
        skip_reasoning = True
        reasoning_markers = [
            '—Ö–æ—Ä–æ—à–æ', '–¥–∞–≤–∞–π—Ç–µ', '—Å–Ω–∞—á–∞–ª–∞', '–Ω—É–∂–Ω–æ', '–≤–æ–∑–º–æ–∂–Ω–æ', '–∏—Ç–∞–∫', '—Ç–µ–ø–µ—Ä—å',
            'well', 'let', 'first', 'need', 'maybe', 'so', 'now', 'then'
        ]
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º reasoning —Å—Ç—Ä–æ–∫–∏
            if skip_reasoning:
                line_lower = line.lower()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏
                if any(line_lower.startswith(marker) for marker in reasoning_markers):
                    continue
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ reasoning-—Ç–µ–∫—Å—Ç
                if len(line) < 30 and any(marker in line_lower for marker in reasoning_markers):
                    continue
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—Ö–æ–∂–µ–µ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –Ω–∞—á–∏–Ω–∞–µ–º —Å–æ–±–∏—Ä–∞—Ç—å
                if len(line) > 15 and not line.startswith('<') and not any(marker in line_lower[:20] for marker in reasoning_markers):
                    skip_reasoning = False
            
            if not skip_reasoning:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —è–≤–ª—è—é—Ç—Å—è reasoning
                line_lower = line.lower()
                if len(line) < 30 and any(marker in line_lower for marker in reasoning_markers):
                    continue
                cleaned_lines.append(line)
        
        result = ' '.join(cleaned_lines).strip()
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥
        if not result or len(result) < 10:
            # –ü—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–Ω–∞—á–∏–º—É—é —Å—Ç—Ä–æ–∫—É
            lines = original_text.split('\n')
            for line in reversed(lines):
                line = line.strip()
                if line and len(line) > 15:
                    line_lower = line.lower()
                    if not any(line_lower.startswith(marker) for marker in reasoning_markers):
                        result = line
                        break
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–∏—á–µ–≥–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ reasoning –º–æ–¥–µ–ª—å)
        if not result or len(result) < 5:
            result = original_text.strip()
        
        return result

    def _call_api(self, prompt: str) -> tuple[str, str]:
        """–í—ã–∑–æ–≤ OpenRouter API
        
        Returns:
            tuple: (raw_response, cleaned_response)
        """
        # OpenRouter –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        request_params = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
            "max_tokens": LLM_MAX_TOKENS,
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —á–µ—Ä–µ–∑ extra_headers –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if LLM_API_ROUTING:
            request_params["extra_headers"] = {"X-OpenRouter-Provider": LLM_API_ROUTING}
        
        response = self.client.chat.completions.create(**request_params)
        raw_response = response.choices[0].message.content
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ reasoning –º–æ–¥–µ–ª–µ–π
        cleaned_response = self._extract_final_answer(raw_response)
        
        return raw_response, cleaned_response
    
    def _preprocess_text_before_llm(self, text: str) -> str:
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—Ç–∞ –∂–µ –ª–æ–≥–∏–∫–∞ —á—Ç–æ –∏ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏)"""
        if not text:
            return ""
        
        patterns_to_remove = [
            r'(?i)(–≥–ª–∞–≤–Ω–∞—è|–Ω–∞–∑–∞–¥|–≤–≤–µ—Ä—Ö|–ø–æ–¥–µ–ª–∏—Ç—å—Å—è|—Å–ª–µ–¥–∏—Ç–µ –∑–∞ –Ω–∞–º–∏|–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è)',
            r'(?i)(–º–µ–Ω—é|–Ω–∞–≤–∏–≥–∞—Ü–∏—è|breadcrumb|—Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏)',
            r'¬©\s*\d{4}[-\s]*\d{4}.*?',
            r'(?i)(–≤—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã|–ª–∏—Ü–µ–Ω–∑–∏—è|–ª–∏—Ü–µ–Ω–∑–∏—è —Ü–± —Ä—Ñ)',
            r'(?i)(—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–¥—Ä–µ—Å|–æ—Ñ–∏—Å|–∫–æ–Ω—Ç–∞–∫—Ç—ã).*?(?=\n\n|\Z)',
            r'(?i)(–æ—Ç–∫—Ä–æ–π—Ç–µ –∫–∞—Ä—Ç—É —Å–µ–≥–æ–¥–Ω—è|—É–∑–Ω–∞–π—Ç–µ –±–æ–ª—å—à–µ|–æ—Å—Ç–∞–≤—å—Ç–µ –∑–∞—è–≤–∫—É|–æ—Ñ–æ—Ä–º–∏—Ç—å –æ–Ω–ª–∞–π–Ω)',
            r'(?i)(—Å–∫–∞—á–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|app store|google play)',
            r'(?i)(–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å|—Ä–∞—Å—Å—ã–ª–∫–∞|–Ω–æ–≤–æ—Å—Ç–∏)',
            r'(?i)(cookie|cookies|–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ cookie)',
            r'(?i)(—Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É|–ø–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏)',
            r'(?i)(–Ω–∞–∂–º–∏—Ç–µ –∑–¥–µ—Å—å|–ø–µ—Ä–µ–π–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ|—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ|—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ)',
            r'(?i)(–ø–æ–¥—Ä–æ–±–Ω–µ–µ|–¥–µ—Ç–∞–ª–∏|—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ)',
            r'[-=]{3,}',
            r'_{3,}',
            r'\n{3,}',
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, ' ', text, flags=re.MULTILINE)
        
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def clean_document(self, text: str) -> Dict:
        """–û—á–∏—Å—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ API"""
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        text_preprocessed = self._preprocess_text_before_llm(text)
        
        if len(text_preprocessed.strip()) < 100:
            return self._fallback_result(text_preprocessed)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        text_hash = hashlib.md5(text_preprocessed[:2000].encode('utf-8')).hexdigest()
        if text_hash in self._cache:
            return self._cache[text_hash].copy()
        
        # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        text_truncated = text_preprocessed[:2500]
        prompt = f"""–û—á–∏—Å—Ç–∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–µ—Ä–Ω–∏ JSON:

–î–û–ö–£–ú–ï–ù–¢:
{text_truncated}

–ó–ê–î–ê–ß–ò:
1. –£–¥–∞–ª–∏: –Ω–∞–≤–∏–≥–∞—Ü–∏—é, —Ñ—É—Ç–µ—Ä—ã, —Ä–µ–∫–ª–∞–º—É, cookie-–±–∞–Ω–Ω–µ—Ä—ã, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
2. –°–æ—Ö—Ä–∞–Ω–∏: –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —á–∏—Å–ª–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, —Å—Ä–æ–∫–∏)
3. –¢–µ–º—ã (–º–∞–∫—Å 3): –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ_–∫–∞—Ä—Ç—ã, –¥–µ–±–µ—Ç–æ–≤—ã–µ_–∫–∞—Ä—Ç—ã, –ø–µ—Ä–µ–≤–æ–¥—ã, –∂–∫—Ö, –∫—ç—à–±—ç–∫, —Å—á–µ—Ç–∞_—Ä–µ–∫–≤–∏–∑–∏—Ç—ã, –∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –º–æ–±–∏–ª—å–Ω–æ–µ_–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –∞–ª—å—Ñ–∞_–æ–Ω–ª–∞–π–Ω, –∏–ø–æ—Ç–µ–∫–∞, –∫—Ä–µ–¥–∏—Ç—ã, –≤–∫–ª–∞–¥—ã, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ
4. –ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: 0.0-0.3 (–º—É—Å–æ—Ä), 0.4-0.6 (—á–∞—Å—Ç–∏—á–Ω–æ), 0.7-1.0 (–∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞)

JSON:
{{
  "clean_text": "–æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
  "topics": ["—Ç–µ–º–∞_1", "—Ç–µ–º–∞_2"],
  "usefulness_score": 0.0
}}"""
        
        # –í—ã–∑–æ–≤ API —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        raw_json_response = None
        for attempt in range(self.retries):
            try:
                raw_response, response_text = self._call_api(prompt)
                raw_json_response = raw_response  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                
                # –£–¥–∞–ª—è–µ–º markdown –∫–æ–¥-–±–ª–æ–∫–∏ (```json ... ```)
                # –ú–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç JSON –≤ markdown —Ñ–æ—Ä–º–∞—Ç–µ
                # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –∏—â–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–µ–∂–¥—É ```json –∏ ```
                json_block_match = re.search(r'```(?:json)?\s*\n(.*?)\n```', response_text, re.DOTALL)
                if json_block_match:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ markdown –±–ª–æ–∫–∞
                    response_text = json_block_match.group(1).strip()
                elif response_text.strip().startswith('```'):
                    # Fallback: –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–º–Ω–æ–≥–æ –¥—Ä—É–≥–æ–π, –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã
                    response_text = re.sub(r'^```(?:json)?\s*\n?', '', response_text, flags=re.MULTILINE)
                    response_text = re.sub(r'\n?```\s*$', '', response_text, flags=re.MULTILINE)
                    response_text = response_text.strip()
                
                # –ü–∞—Ä—Å–∏–Ω–≥ JSON (—É–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π reasoning)
                raw_result = None
                
                # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –∏—â–µ–º –ø–µ—Ä–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç, –Ω–∞—á–∏–Ω–∞—è —Å –ø–µ—Ä–≤–æ–π {
                first_brace = response_text.find('{')
                if first_brace != -1:
                    brace_count = 0
                    last_brace = -1
                    for i in range(first_brace, len(response_text)):
                        if response_text[i] == '{':
                            brace_count += 1
                        elif response_text[i] == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                last_brace = i
                                break
                    
                    if last_brace != -1:
                        try:
                            json_str = response_text[first_brace:last_brace + 1]
                            raw_result = json.loads(json_str)
                        except json.JSONDecodeError:
                            pass
                
                # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –º–µ–∂–¥—É –ø–µ—Ä–≤—ã–º–∏ { –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ }
                if raw_result is None:
                    first_brace = response_text.find('{')
                    last_brace = response_text.rfind('}')
                    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                        try:
                            json_str = response_text[first_brace:last_brace + 1]
                            raw_result = json.loads(json_str)
                        except json.JSONDecodeError:
                            pass
                
                # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —Ç–∞–º —Ç–æ–ª—å–∫–æ JSON)
                if raw_result is None:
                    try:
                        raw_result = json.loads(response_text.strip())
                    except json.JSONDecodeError:
                        pass
                
                # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –ø–æ—Å–ª–µ reasoning –º–∞—Ä–∫–µ—Ä–æ–≤
                if raw_result is None:
                    # –ò—â–µ–º JSON –ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫ —Ç–∏–ø–∞ "JSON:", "–û—Ç–≤–µ—Ç:", "–†–µ–∑—É–ª—å—Ç–∞—Ç:"
                    json_markers = ['json:', '–æ—Ç–≤–µ—Ç:', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç:', 'result:', 'output:']
                    for marker in json_markers:
                        marker_pos = response_text.lower().find(marker)
                        if marker_pos != -1:
                            # –ò—â–µ–º { –ø–æ—Å–ª–µ –º–∞—Ä–∫–µ—Ä–∞
                            brace_pos = response_text.find('{', marker_pos)
                            if brace_pos != -1:
                                try:
                                    # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å JSON –æ—Ç —ç—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏
                                    brace_count = 0
                                    last_brace = -1
                                    for i in range(brace_pos, len(response_text)):
                                        if response_text[i] == '{':
                                            brace_count += 1
                                        elif response_text[i] == '}':
                                            brace_count -= 1
                                            if brace_count == 0:
                                                last_brace = i
                                                break
                                    if last_brace != -1:
                                        json_str = response_text[brace_pos:last_brace + 1]
                                        raw_result = json.loads(json_str)
                                        break
                                except json.JSONDecodeError:
                                    continue
                
                if raw_result:
                    raw_result.setdefault("clean_text", text_truncated)
                    raw_result.setdefault("topics", [])
                    raw_result.setdefault("usefulness_score", 0.5)
                    raw_result.setdefault("products", [])
                    raw_result.setdefault("actions", [])
                    raw_result.setdefault("conditions", [])
                    raw_result["is_useful"] = bool(raw_result.get("usefulness_score", 0.5) >= 0.3)
                    
                    # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
                    if len(self._cache) >= self._cache_max_size:
                        oldest_key = next(iter(self._cache))
                        del self._cache[oldest_key]
                    self._cache[text_hash] = raw_result.copy()
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Å—ã—Ä—ã–º JSON –æ—Ç–≤–µ—Ç–æ–º
                    self._log_llm_result(raw_result, original_text=text_truncated, raw_json_response=raw_json_response)
                    
                    return raw_result
                else:
                    # Fallback –µ—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
                    fallback = self._fallback_result(text_truncated)
                    self._log_llm_result(fallback, original_text=text_truncated, reason="json_parse_failed", raw_json_response=raw_json_response)
                    return fallback
            
            except Exception as e:
                if attempt < self.retries - 1:
                    if self.verbose:
                        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ API (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.retries}): {e}")
                    continue
                else:
                    if self.verbose:
                        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ API –ø–æ—Å–ª–µ {self.retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                    fallback = self._fallback_result(text_truncated)
                    self._log_llm_result(fallback, original_text=text_truncated, reason=str(e), raw_json_response=raw_json_response)
                    return fallback
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        fallback = self._fallback_result(text_truncated)
        self._log_llm_result(fallback, original_text=text_truncated, reason="all_retries_exhausted", raw_json_response=raw_json_response)
        return fallback
    
    def _fallback_result(self, text: str) -> Dict:
        """Fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª"""
        return {
            "clean_text": text,
            "topics": [],
            "usefulness_score": 0.5,
            "products": [],
            "actions": [],
            "conditions": [],
            "is_useful": True
        }
    
    def _log_llm_result(self, result: Dict, original_text: str, reason: Optional[str] = None, raw_json_response: Optional[str] = None) -> None:
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ LLM –æ—á–∏—Å—Ç–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON-–ª–æ–≥.
        
        Args:
            result: —Ä–µ–∑—É–ª—å—Ç–∞—Ç clean_document
            original_text: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            reason: –ø—Ä–∏—á–∏–Ω–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "json_parse_failed")
            raw_json_response: —Å—ã—Ä–æ–π JSON –æ—Ç–≤–µ—Ç –æ—Ç API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª–æ–≥–≥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
        if not self.llm_logger.handlers:
            # –õ–æ–≥–≥–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª)
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            try:
                self._init_llm_logger()
            except Exception:
                pass
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Ö–µ–Ω–¥–ª–µ—Ä–æ–≤ - –≤—ã—Ö–æ–¥–∏–º
            if not self.llm_logger.handlers:
                return

        try:
            log_record = {
                "reason": reason,
                "usefulness_score": result.get("usefulness_score"),
                "is_useful": result.get("is_useful"),
                "topics": result.get("topics", []),
                "products": result.get("products", []),
                "actions": result.get("actions", []),
                "conditions": result.get("conditions", []),
                # web_id –º–æ–∂–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–∞—Ö ‚Äî –∑–¥–µ—Å—å –æ–±—ã—á–Ω–æ None,
                # –Ω–æ –ø–æ–ª–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è –µ—Å–ª–∏ –≤ –±—É–¥—É—â–µ–º —Ç—É–¥–∞ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å
                "web_id": result.get("web_id"),
                # –ü—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–æ–≤ (—É—Å–µ–∫–∞–µ–º, —á—Ç–æ–±—ã —Ñ–∞–π–ª –±—ã–ª —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
                "original_text_preview": original_text[:1000],
                "clean_text_preview": str(result.get("clean_text", ""))[:1000],
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—ã—Ä–æ–π JSON –æ—Ç–≤–µ—Ç –æ—Ç API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if raw_json_response is not None:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å—ã—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤)
                log_record["raw_json_response"] = raw_json_response[:2000]
            
            self.llm_logger.info(json.dumps(log_record, ensure_ascii=False))
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä—ã –≤—Å–µ—Ö —Ö–µ–Ω–¥–ª–µ—Ä–æ–≤
            for handler in self.llm_logger.handlers:
                handler.flush()
        except Exception as e:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –ª–æ–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
            # –ù–æ –º–æ–∂–µ–º –≤—ã–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –≤ verbose —Ä–µ–∂–∏–º–µ
            if self.verbose:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
            pass


class LLMDocumentCleaner:
    """
    LLM-based –æ—á–∏—Å—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen3-32B (–∏–ª–∏ –¥—Ä—É–≥—É—é LLM) –¥–ª—è:
    - –£–¥–∞–ª–µ–Ω–∏—è –º—É—Å–æ—Ä–∞ –∏–∑ –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –î–æ–±–∞–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    """

    def __init__(self, model_path: Optional[str] = None, verbose: bool = True, n_workers: Optional[int] = None):
        """
        Args:
            model_path: –ø—É—Ç—å –∫ GGUF –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑ config)
            verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
            n_workers: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM_PARALLEL_WORKERS –∏–∑ config)
        """
        if model_path is None:
            model_path = str(MODELS_DIR / LLM_MODEL_FILE)

        self.model_path = model_path
        self.verbose = verbose
        self.llm = None

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
        self.n_workers = n_workers if n_workers is not None else LLM_PARALLEL_WORKERS

        # –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ —Ö—ç—à—É —Ç–µ–∫—Å—Ç–∞)
        # –ö—ç—à–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        self._cache = {}
        self._cache_max_size = 100
        self._cache_lock = threading.Lock()  # Lock –¥–ª—è thread-safe –¥–æ—Å—Ç—É–ø–∞ –∫ –∫—ç—à—É

        # –û—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–±–æ—Ç—ã LLM
        # (—á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–µ—Ä–Ω—É–ª–∞ –º–æ–¥–µ–ª—å)
        self.llm_logger = logging.getLogger("llm_cleaning")
        self._init_llm_logger()

        if verbose:
            print(f"\n{'='*80}")
            print(f"üì• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Document Cleaner")
            print(f"   –ú–æ–¥–µ–ª—å: {Path(model_path).name}")
            print(f"   –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤: {self.n_workers}")
            print(f"{'='*80}\n")

    def _init_llm_logger(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ª–æ–≥-—Ñ–∞–π–ª–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM –æ—á–∏—Å—Ç–∫–∏.

        –§–æ—Ä–º–∞—Ç: –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º clean_document.
        """
        try:
            OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
        except Exception:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é ‚Äî —Ç–∏—Ö–æ –≤—ã—Ö–æ–¥–∏–º, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω
            return

        log_path = OUTPUTS_DIR / "llm_cleaning.log"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –ª–∏ —É–∂–µ —Ö–µ–Ω–¥–ª–µ—Ä –Ω–∞ —ç—Ç–æ—Ç —Ñ–∞–π–ª
        handler_exists = any(
            isinstance(h, RotatingFileHandler) and getattr(h, "baseFilename", None) == str(log_path)
            for h in self.llm_logger.handlers
        )
        if handler_exists:
            return

        handler = RotatingFileHandler(
            str(log_path),
            maxBytes=25 * 1024 * 1024,
            backupCount=3,
            encoding="utf-8",
        )
        # –õ–æ–≥–∏—Ä—É–µ–º –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–µ (JSON)
        formatter = logging.Formatter("%(message)s")
        handler.setFormatter(formatter)

        # INFO –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Ç.–∫. –∫–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å ‚Äî –æ–¥–∏–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç LLM
        handler.setLevel(logging.INFO)
        self.llm_logger.setLevel(logging.INFO)
        self.llm_logger.addHandler(handler)
        # –ù–µ –¥—É–±–ª–∏—Ä—É–µ–º –≤ root, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        self.llm_logger.propagate = False
        
        # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ (—Ç–æ–ª—å–∫–æ –≤ verbose —Ä–µ–∂–∏–º–µ)
        if self.verbose:
            print(f"  üìù LLM –ª–æ–≥-—Ñ–∞–π–ª: {log_path}")
            print(f"     –•–µ–Ω–¥–ª–µ—Ä–æ–≤: {len(self.llm_logger.handlers)}")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ LLM –º–æ–¥–µ–ª–∏"""
        if self.llm is not None:
            return  # —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

        model_path = Path(self.model_path)

        if not model_path.exists():
            raise FileNotFoundError(
                f"LLM –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\n"
                f"–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: python download_models.py"
            )

        if self.verbose:
            print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")

        self.llm = Llama(
            model_path=str(model_path),
            n_ctx=LLM_CONTEXT_SIZE,
            n_gpu_layers=LLM_GPU_LAYERS,
            n_batch=LLM_N_BATCH,  # –∏–∑ config (1024 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
            n_threads=LLM_N_THREADS,  # –∏–∑ config (16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
            use_mlock=True,
            verbose=False
        )

        if self.verbose:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def _preprocess_text_before_llm(self, text: str) -> str:
        """
        –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ LLM –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è.
        –£–¥–∞–ª—è–µ—Ç –æ—á–µ–≤–∏–¥–Ω—ã–π –º—É—Å–æ—Ä —á–µ—Ä–µ–∑ regex, —á—Ç–æ–±—ã —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM.
        
        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not text:
            return ""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –æ—á–µ–≤–∏–¥–Ω–æ–≥–æ –º—É—Å–æ—Ä–∞
        patterns_to_remove = [
            # –ù–∞–≤–∏–≥–∞—Ü–∏—è –∏ –º–µ–Ω—é
            r'(?i)(–≥–ª–∞–≤–Ω–∞—è|–Ω–∞–∑–∞–¥|–≤–≤–µ—Ä—Ö|–ø–æ–¥–µ–ª–∏—Ç—å—Å—è|—Å–ª–µ–¥–∏—Ç–µ –∑–∞ –Ω–∞–º–∏|–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è)',
            r'(?i)(–º–µ–Ω—é|–Ω–∞–≤–∏–≥–∞—Ü–∏—è|breadcrumb|—Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏)',
            
            # –§—É—Ç–µ—Ä—ã –∏ –∫–æ–ø–∏—Ä–∞–π—Ç—ã
            r'¬©\s*\d{4}[-\s]*\d{4}.*?',
            r'(?i)(–≤—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã|–ª–∏—Ü–µ–Ω–∑–∏—è|–ª–∏—Ü–µ–Ω–∑–∏—è —Ü–± —Ä—Ñ)',
            r'(?i)(—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–¥—Ä–µ—Å|–æ—Ñ–∏—Å|–∫–æ–Ω—Ç–∞–∫—Ç—ã).*?(?=\n\n|\Z)',
            
            # –†–µ–∫–ª–∞–º–∞ –∏ –ø—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é
            r'(?i)(–æ—Ç–∫—Ä–æ–π—Ç–µ –∫–∞—Ä—Ç—É —Å–µ–≥–æ–¥–Ω—è|—É–∑–Ω–∞–π—Ç–µ –±–æ–ª—å—à–µ|–æ—Å—Ç–∞–≤—å—Ç–µ –∑–∞—è–≤–∫—É|–æ—Ñ–æ—Ä–º–∏—Ç—å –æ–Ω–ª–∞–π–Ω)',
            r'(?i)(—Å–∫–∞—á–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|app store|google play)',
            r'(?i)(–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å|—Ä–∞—Å—Å—ã–ª–∫–∞|–Ω–æ–≤–æ—Å—Ç–∏)',
            
            # Cookie –∏ –±–∞–Ω–Ω–µ—Ä—ã
            r'(?i)(cookie|cookies|–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ cookie)',
            r'(?i)(—Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É|–ø–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏)',
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
            r'(?i)(–Ω–∞–∂–º–∏—Ç–µ –∑–¥–µ—Å—å|–ø–µ—Ä–µ–π–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ|—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ|—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ)',
            r'(?i)(–ø–æ–¥—Ä–æ–±–Ω–µ–µ|–¥–µ—Ç–∞–ª–∏|—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ)',
            
            # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            r'[-=]{3,}',
            r'_{3,}',
            
            # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            r'\n{3,}',
        ]
        
        # –£–¥–∞–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in patterns_to_remove:
            text = re.sub(pattern, ' ', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏ (–µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å)
        text = re.sub(r'<[^>]+>', '', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text

    def clean_document(self, text: str) -> Dict:
        """
        –û—á–∏—Å—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM

        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            dict —Å –ø–æ–ª—è–º–∏:
                - clean_text: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                - products: —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                - actions: —Å–ø–∏—Å–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π
                - conditions: —Å–ø–∏—Å–æ–∫ —É—Å–ª–æ–≤–∏–π
                - topics: —Å–ø–∏—Å–æ–∫ —Ç–µ–º
                - usefulness_score: –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (0-1)
                - is_useful: bool
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–¥ LLM (—É–¥–∞–ª—è–µ–º –æ—á–µ–≤–∏–¥–Ω—ã–π –º—É—Å–æ—Ä)
        text_preprocessed = self._preprocess_text_before_llm(text)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏) - —ç–∫–æ–Ω–æ–º–∏–º –≤—Ä–µ–º—è LLM
        if len(text_preprocessed.strip()) < 100:
            fallback = self._fallback_result(text_preprocessed)
            self._log_llm_result(fallback, original_text=text, reason="too_short_after_preprocessing")
            return fallback
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (–ø–æ —Ö—ç—à—É –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞) - thread-safe
        text_hash = hashlib.md5(text_preprocessed[:2000].encode('utf-8')).hexdigest()
        with self._cache_lock:
            if text_hash in self._cache:
                cached_result = self._cache[text_hash].copy()
                self._log_llm_result(cached_result, original_text=text, reason="cached")
                return cached_result
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        text_truncated = text_preprocessed[:2500]  # –±—ã–ª–æ 3000, —É–º–µ–Ω—å—à–µ–Ω–æ —Ç.–∫. —É–∂–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ

        # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (—É–±—Ä–∞–Ω—ã –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è)
        prompt = f"""–û—á–∏—Å—Ç–∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–µ—Ä–Ω–∏ JSON:

–î–û–ö–£–ú–ï–ù–¢:
{text_truncated}

–ó–ê–î–ê–ß–ò:
1. –£–¥–∞–ª–∏: –Ω–∞–≤–∏–≥–∞—Ü–∏—é, —Ñ—É—Ç–µ—Ä—ã, —Ä–µ–∫–ª–∞–º—É, cookie-–±–∞–Ω–Ω–µ—Ä—ã, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
2. –°–æ—Ö—Ä–∞–Ω–∏: –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —á–∏—Å–ª–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, —Å—Ä–æ–∫–∏)
3. –¢–µ–º—ã (–º–∞–∫—Å 3): –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ_–∫–∞—Ä—Ç—ã, –¥–µ–±–µ—Ç–æ–≤—ã–µ_–∫–∞—Ä—Ç—ã, –ø–µ—Ä–µ–≤–æ–¥—ã, –∂–∫—Ö, –∫—ç—à–±—ç–∫, —Å—á–µ—Ç–∞_—Ä–µ–∫–≤–∏–∑–∏—Ç—ã, –∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –º–æ–±–∏–ª—å–Ω–æ–µ_–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –∞–ª—å—Ñ–∞_–æ–Ω–ª–∞–π–Ω, –∏–ø–æ—Ç–µ–∫–∞, –∫—Ä–µ–¥–∏—Ç—ã, –≤–∫–ª–∞–¥—ã, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ
4. –ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: 0.0-0.3 (–º—É—Å–æ—Ä), 0.4-0.6 (—á–∞—Å—Ç–∏—á–Ω–æ), 0.7-1.0 (–∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞)

JSON:
{{
  "clean_text": "–æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
  "topics": ["—Ç–µ–º–∞_1", "—Ç–µ–º–∞_2"],
  "usefulness_score": 0.0
}}"""

        try:
            response = self.llm(
                prompt,
                max_tokens=LLM_MAX_TOKENS,  # –∏–∑ config.py (1024 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
                temperature=0.1,
                stop=["<|im_end|>"],
                top_p=0.9,  # nucleus sampling –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                top_k=40,  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            )

            response_text = response['choices'][0]['text']

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
            # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π:
            raw_result = None
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –∏—â–µ–º –ø–µ—Ä–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç, –Ω–∞—á–∏–Ω–∞—è —Å –ø–µ—Ä–≤–æ–π {
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∂–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–≥–æ JSON –æ–±—ä–µ–∫—Ç–∞
            first_brace = response_text.find('{')
            if first_brace != -1:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ–∑–∏—Ü–∏–∏ –ø–µ—Ä–≤–æ–π {
                # –ò–¥–µ–º –æ—Ç –∫–æ–Ω—Ü–∞ –∫ –Ω–∞—á–∞–ª—É, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
                brace_count = 0
                last_brace = -1
                for i in range(first_brace, len(response_text)):
                    if response_text[i] == '{':
                        brace_count += 1
                    elif response_text[i] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_brace = i
                            break
                
                if last_brace != -1:
                    try:
                        json_str = response_text[first_brace:last_brace + 1]
                        raw_result = json.loads(json_str)
                    except json.JSONDecodeError:
                        pass
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –º–µ–∂–¥—É –ø–µ—Ä–≤—ã–º–∏ { –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ }
            if raw_result is None:
                first_brace = response_text.find('{')
                last_brace = response_text.rfind('}')
                if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                    try:
                        json_str = response_text[first_brace:last_brace + 1]
                        raw_result = json.loads(json_str)
                    except json.JSONDecodeError:
                        pass
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —Ç–∞–º —Ç–æ–ª—å–∫–æ JSON)
            if raw_result is None:
                try:
                    raw_result = json.loads(response_text.strip())
                except json.JSONDecodeError:
                    pass
            
            if raw_result:
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–µ–π –ø–æ–¥ downstream:
                # –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏
                raw_result.setdefault("clean_text", text_truncated)
                raw_result.setdefault("topics", [])
                raw_result.setdefault("usefulness_score", 0.5)
                # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: —ç—Ç–∏ –ø–æ–ª—è –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è downstream (entities —Å–±–æ—Ä–∫–∞)
                raw_result.setdefault("products", [])
                raw_result.setdefault("actions", [])
                raw_result.setdefault("conditions", [])
                # derive is_useful –ø–æ –ø—Ä–µ–∂–Ω–µ–π –ª–æ–≥–∏–∫–µ (–ø–æ—Ä–æ–≥ ~0.3)
                raw_result["is_useful"] = bool(raw_result.get("usefulness_score", 0.5) >= 0.3)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä) - thread-safe
                with self._cache_lock:
                    if len(self._cache) >= self._cache_max_size:
                        # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç (FIFO)
                        oldest_key = next(iter(self._cache))
                        del self._cache[oldest_key]
                    self._cache[text_hash] = raw_result.copy()

                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥-—Ñ–∞–π–ª (–±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —à—É–º–∞)
                self._log_llm_result(raw_result, original_text=text_truncated)

                return raw_result
            else:
                # Fallback –µ—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
                fallback = self._fallback_result(text_truncated)
                self._log_llm_result(fallback, original_text=text_truncated, reason="json_parse_failed")
                return fallback

        except Exception as e:
            if self.verbose:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            fallback = self._fallback_result(text_truncated)
            self._log_llm_result(fallback, original_text=text_truncated, reason=str(e))
            return fallback

    def _fallback_result(self, text: str) -> Dict:
        """Fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ LLM –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        return {
            "clean_text": text,
            "products": [],
            "actions": [],
            "conditions": [],
            "topics": [],
            "usefulness_score": 0.5,
            "is_useful": True
        }

    def _log_llm_result(self, result: Dict, original_text: str, reason: Optional[str] = None, raw_json_response: Optional[str] = None) -> None:
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ LLM –æ—á–∏—Å—Ç–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON-–ª–æ–≥.

        –ú—ã –ª–æ–≥–∏—Ä—É–µ–º:
        - –∫—Ä–∞—Ç–∫–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ,
        - —É—Å–µ—á—ë–Ω–Ω—ã–π original_text –∏ clean_text (—á—Ç–æ–±—ã –ª–æ–≥ –Ω–µ —Ä–∞–∑—Ä–∞—Å—Ç–∞–ª—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ).
        """
        if not self.llm_logger.handlers:
            # –õ–æ–≥–≥–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª)
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            try:
                self._init_llm_logger()
            except Exception:
                pass
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Ö–µ–Ω–¥–ª–µ—Ä–æ–≤ - –≤—ã—Ö–æ–¥–∏–º
            if not self.llm_logger.handlers:
                return

        try:
            log_record = {
                "reason": reason,
                "usefulness_score": result.get("usefulness_score"),
                "is_useful": result.get("is_useful"),
                "topics": result.get("topics", []),
                "products": result.get("products", []),
                "actions": result.get("actions", []),
                "conditions": result.get("conditions", []),
                # web_id –º–æ–∂–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–∞—Ö ‚Äî –∑–¥–µ—Å—å –æ–±—ã—á–Ω–æ None,
                # –Ω–æ –ø–æ–ª–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è –µ—Å–ª–∏ –≤ –±—É–¥—É—â–µ–º —Ç—É–¥–∞ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å
                "web_id": result.get("web_id"),
                # –ü—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–æ–≤ (—É—Å–µ–∫–∞–µ–º, —á—Ç–æ–±—ã —Ñ–∞–π–ª –±—ã–ª —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
                "original_text_preview": original_text[:1000],
                "clean_text_preview": str(result.get("clean_text", ""))[:1000],
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—ã—Ä–æ–π JSON –æ—Ç–≤–µ—Ç –æ—Ç API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if raw_json_response is not None:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å—ã—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤)
                log_record["raw_json_response"] = raw_json_response[:2000]
            self.llm_logger.info(json.dumps(log_record, ensure_ascii=False))
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä—ã –≤—Å–µ—Ö —Ö–µ–Ω–¥–ª–µ—Ä–æ–≤
            for handler in self.llm_logger.handlers:
                handler.flush()
        except Exception as e:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –ª–æ–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
            # –ù–æ –º–æ–∂–µ–º –≤—ã–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –≤ verbose —Ä–µ–∂–∏–º–µ
            if self.verbose:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
            pass

    def process_documents(self, documents_df: pd.DataFrame,
                         text_column: str = 'text') -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏:
                - clean_text
                - products (JSON list)
                - actions (JSON list)
                - conditions (JSON list)
                - topics (JSON list)
                - usefulness_score (float)
                - is_useful (bool)
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        results = []

        if self.verbose:
            print(f"\nüöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents_df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM...")
            iterator = tqdm(documents_df.iterrows(), total=len(documents_df), desc="LLM Cleaning")
        else:
            iterator = documents_df.iterrows()

        for idx, row in iterator:
            text = row[text_column]

            # –û—á–∏—â–∞–µ–º —á–µ—Ä–µ–∑ LLM
            cleaned = self.clean_document(text)

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ + –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            result_row = {
                **row.to_dict(),
                'clean_text': cleaned.get('clean_text', text),
                'products': json.dumps(cleaned.get('products', []), ensure_ascii=False),
                'actions': json.dumps(cleaned.get('actions', []), ensure_ascii=False),
                'conditions': json.dumps(cleaned.get('conditions', []), ensure_ascii=False),
                'topics': json.dumps(cleaned.get('topics', []), ensure_ascii=False),
                'usefulness_score': cleaned.get('usefulness_score', 0.5),
                'is_useful': cleaned.get('is_useful', True)
            }

            results.append(result_row)

        result_df = pd.DataFrame(results)

        if self.verbose:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            useful_count = result_df['is_useful'].sum()
            avg_score = result_df['usefulness_score'].mean()

            print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
            print(f"   –ü–æ–ª–µ–∑–Ω—ã—Ö: {useful_count}/{len(result_df)} ({useful_count/len(result_df)*100:.1f}%)")
            print(f"   –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.2f}")

            # –¢–æ–ø —Ç–µ–º—ã
            all_topics = []
            for topics_json in result_df['topics']:
                try:
                    topics = json.loads(topics_json)
                    all_topics.extend(topics)
                except:
                    pass

            if all_topics:
                from collections import Counter
                topic_counts = Counter(all_topics)

                print(f"\nüìä –¢–æ–ø-5 —Ç–µ–º:")
                for topic, count in topic_counts.most_common(5):
                    print(f"   {topic}: {count}")

        return result_df

    def filter_by_usefulness(self, documents_df: pd.DataFrame,
                            min_score: float = 0.3) -> pd.DataFrame:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–ø–æ—Å–ª–µ process_documents)
            min_score: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ usefulness_score

        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        before_count = len(documents_df)

        filtered_df = documents_df[
            (documents_df['is_useful'] == True) &
            (documents_df['usefulness_score'] >= min_score)
        ].copy()

        after_count = len(filtered_df)
        removed_count = before_count - after_count

        if self.verbose:
            print(f"\nüóëÔ∏è  –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (min_score={min_score}):")
            print(f"   –ë—ã–ª–æ: {before_count}")
            print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {after_count}")
            print(f"   –£–¥–∞–ª–µ–Ω–æ: {removed_count} ({removed_count/before_count*100:.1f}%)")

        return filtered_df


def apply_llm_cleaning(documents_df: pd.DataFrame,
                      min_usefulness: float = 0.3,
                      verbose: bool = True) -> pd.DataFrame:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM –æ—á–∏—Å—Ç–∫–∏

    Args:
        documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        min_usefulness: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π DataFrame
    """
    cleaner = LLMDocumentCleaner(verbose=verbose)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    cleaned_df = cleaner.process_documents(documents_df)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if min_usefulness > 0:
        cleaned_df = cleaner.filter_by_usefulness(cleaned_df, min_score=min_usefulness)

    return cleaned_df


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
    print("="*80)
    print("–¢–ï–°–¢ LLM DOCUMENT CLEANER")
    print("="*80)

    # –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    test_doc = """
    –ì–ª–∞–≤–Ω–∞—è / –ö–∞—Ä—Ç—ã / –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞

    –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞ - –¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å –∫—ç—à–±—ç–∫–æ–º

    –ü–æ–ª—É—á–∞–π—Ç–µ –∫—ç—à–±—ç–∫ 2% –Ω–∞ –≤—Å–µ –ø–æ–∫—É–ø–∫–∏ –∏ –¥–æ 10% –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –Ω–∞ –≤—ã–±–æ—Ä.
    –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–∏ —Å—É–º–º–µ –ø–æ–∫—É–ø–æ–∫ –æ—Ç 10000 —Ä—É–±–ª–µ–π –≤ –º–µ—Å—è—Ü.

    –û—Ñ–æ—Ä–º–∏—Ç—å –æ–Ω–ª–∞–π–Ω

    ¬© 2001-2025 –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫. –õ–∏—Ü–µ–Ω–∑–∏—è –¶–ë –†–§ ‚Ññ1326
    """

    test_df = pd.DataFrame([{'text': test_doc, 'web_id': 1}])

    cleaner = LLMDocumentCleaner(verbose=True)
    result_df = cleaner.process_documents(test_df)

    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢:")
    print("="*80)
    print(f"\n–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{result_df.iloc[0]['clean_text']}")
    print(f"\n–ü—Ä–æ–¥—É–∫—Ç—ã: {result_df.iloc[0]['products']}")
    print(f"–¢–µ–º—ã: {result_df.iloc[0]['topics']}")
    print(f"–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: {result_df.iloc[0]['usefulness_score']}")
