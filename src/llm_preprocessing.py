"""
LLM-based –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ main_pipeline.py —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ --llm-clean

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:
1. –£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä (–Ω–∞–≤–∏–≥–∞—Ü–∏—è, —Ñ—É—Ç–µ—Ä—ã, —Ä–µ–∫–ª–∞–º–∞) —á–µ—Ä–µ–∑ LLM
2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, —Ç–µ—Ä–º–∏–Ω—ã)
3. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
4. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å
5. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
"""
import pandas as pd
from llama_cpp import Llama
from tqdm import tqdm
import json
import re
from pathlib import Path
from typing import Dict, Optional

from src.config import (
    LLM_MODEL_FILE,
    LLM_CONTEXT_SIZE,
    LLM_GPU_LAYERS,
    MODELS_DIR
)


class LLMDocumentCleaner:
    """
    LLM-based –æ—á–∏—Å—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen3-32B (–∏–ª–∏ –¥—Ä—É–≥—É—é LLM) –¥–ª—è:
    - –£–¥–∞–ª–µ–Ω–∏—è –º—É—Å–æ—Ä–∞ –∏–∑ –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –î–æ–±–∞–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    """

    def __init__(self, model_path: Optional[str] = None, verbose: bool = True):
        """
        Args:
            model_path: –ø—É—Ç—å –∫ GGUF –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑ config)
            verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        """
        if model_path is None:
            model_path = str(MODELS_DIR / LLM_MODEL_FILE)

        self.model_path = model_path
        self.verbose = verbose
        self.llm = None

        if verbose:
            print(f"\n{'='*80}")
            print(f"üì• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Document Cleaner")
            print(f"   –ú–æ–¥–µ–ª—å: {Path(model_path).name}")
            print(f"{'='*80}\n")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ LLM –º–æ–¥–µ–ª–∏"""
        if self.llm is not None:
            return  # —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

        model_path = Path(self.model_path)

        if not model_path.exists():
            raise FileNotFoundError(
                f"LLM –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\n"
                f"–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: python download_models.py"
            )

        if self.verbose:
            print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")

        self.llm = Llama(
            model_path=str(model_path),
            n_ctx=LLM_CONTEXT_SIZE,
            n_gpu_layers=LLM_GPU_LAYERS,
            n_batch=512,
            n_threads=8,
            use_mlock=True,
            verbose=False
        )

        if self.verbose:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def clean_document(self, text: str) -> Dict:
        """
        –û—á–∏—Å—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM

        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            dict —Å –ø–æ–ª—è–º–∏:
                - clean_text: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                - products: —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                - actions: —Å–ø–∏—Å–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π
                - conditions: —Å–ø–∏—Å–æ–∫ —É—Å–ª–æ–≤–∏–π
                - topics: —Å–ø–∏—Å–æ–∫ —Ç–µ–º
                - usefulness_score: –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (0-1)
                - is_useful: bool
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        text_truncated = text[:4000]

        prompt = f"""<|im_start|>system
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞:

1. –£–î–ê–õ–ò–¢–¨ –≤–µ—Å—å –º—É—Å–æ—Ä:
   - –ù–∞–≤–∏–≥–∞—Ü–∏—é (–º–µ–Ω—é, —Å—Å—ã–ª–∫–∏ "–ü–æ–¥–µ–ª–∏—Ç—å—Å—è", "–ù–∞–∑–∞–¥", —Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏)
   - –§—É—Ç–µ—Ä—ã (¬© 2001-2025, –∞–¥—Ä–µ—Å–∞ –æ—Ñ–∏—Å–æ–≤, –ª–∏—Ü–µ–Ω–∑–∏–∏)
   - –†–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏ ("–û—Ç–∫—Ä–æ–π—Ç–µ –∫–∞—Ä—Ç—É —Å–µ–≥–æ–¥–Ω—è!")
   - Cookie-–±–∞–Ω–Ω–µ—Ä—ã –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
   - –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —á–∞—Å—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
   - –°–ø–∏—Å–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –±–µ–∑ –¥–µ—Ç–∞–ª–µ–π

2. –ò–ó–í–õ–ï–ß–¨ –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:
   - –ü—Ä–æ–¥—É–∫—Ç—ã/—É—Å–ª—É–≥–∏: –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞, –ê-–ö–ª—É–±, –∏–ø–æ—Ç–µ–∫–∞, –≤–∫–ª–∞–¥—ã –∏ —Ç.–¥.
   - –î–µ–π—Å—Ç–≤–∏—è: –æ–ø–ª–∞—Ç–∞ –ñ–ö–•, –ø–µ—Ä–µ–≤–æ–¥, –æ—Ç–∫—Ä—ã—Ç–∏–µ —Å—á–µ—Ç–∞, –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–∫–∏
   - –£—Å–ª–æ–≤–∏—è: –∫–æ–º–∏—Å—Å–∏–∏ (0%, 1.5%), –ª–∏–º–∏—Ç—ã (100000), —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–æ—Ç 18 –ª–µ—Ç)

3. –û–ü–†–ï–î–ï–õ–ò–¢–¨ —Ç–µ–º—ã (–º–∞–∫—Å–∏–º—É–º 3):
   –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ_–∫–∞—Ä—Ç—ã, –¥–µ–±–µ—Ç–æ–≤—ã–µ_–∫–∞—Ä—Ç—ã, –ø–µ—Ä–µ–≤–æ–¥—ã, –∂–∫—Ö, –∫—ç—à–±—ç–∫,
   —Å—á–µ—Ç–∞_—Ä–µ–∫–≤–∏–∑–∏—Ç—ã, –∫–æ–º–∏—Å—Å–∏–∏, –ª–∏–º–∏—Ç—ã, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å,
   –º–æ–±–∏–ª—å–Ω–æ–µ_–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –∞–ª—å—Ñ–∞_–æ–Ω–ª–∞–π–Ω, –∏–ø–æ—Ç–µ–∫–∞, –∫—Ä–µ–¥–∏—Ç—ã,
   –≤–∫–ª–∞–¥—ã, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ

4. –û–¶–ï–ù–ò–¢–¨ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å:
   - 0.0-0.3: –º—É—Å–æ—Ä (—Ç–æ–ª—å–∫–æ –Ω–∞–≤–∏–≥–∞—Ü–∏—è/—Ä–µ–∫–ª–∞–º–∞/–æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã)
   - 0.4-0.6: —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–ª–µ–∑–Ω–æ (–µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –Ω–æ –Ω–µ–ø–æ–ª–Ω–∞—è)
   - 0.7-1.0: –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è/—É—Å–ª–æ–≤–∏—è)
<|im_end|>
<|im_start|>user
–î–æ–∫—É–º–µ–Ω—Ç:
{text_truncated}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
{{
  "clean_text": "–æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –º—É—Å–æ—Ä–∞, —Ç–æ–ª—å–∫–æ —Å—É—Ç—å",
  "products": ["–ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞", "–ê-–ö–ª—É–±"],
  "actions": ["–æ–ø–ª–∞—Ç–∞ –ñ–ö–•", "–ø–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–±—ç–∫–∞"],
  "conditions": ["–∫–æ–º–∏—Å—Å–∏—è 0%", "–ª–∏–º–∏—Ç 15000"],
  "topics": ["–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ_–∫–∞—Ä—Ç—ã", "–∫—ç—à–±—ç–∫"],
  "usefulness_score": 0.8,
  "is_useful": true
}}
<|im_end|>
<|im_start|>assistant
"""

        try:
            response = self.llm(
                prompt,
                max_tokens=2048,
                temperature=0.1,
                stop=["<|im_end|>"],
            )

            response_text = response['choices'][0]['text']

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(0))
                return result
            else:
                # Fallback –µ—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
                return self._fallback_result(text_truncated)

        except Exception as e:
            if self.verbose:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return self._fallback_result(text_truncated)

    def _fallback_result(self, text: str) -> Dict:
        """Fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ LLM –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        return {
            "clean_text": text,
            "products": [],
            "actions": [],
            "conditions": [],
            "topics": [],
            "usefulness_score": 0.5,
            "is_useful": True
        }

    def process_documents(self, documents_df: pd.DataFrame,
                         text_column: str = 'text') -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏:
                - clean_text
                - products (JSON list)
                - actions (JSON list)
                - conditions (JSON list)
                - topics (JSON list)
                - usefulness_score (float)
                - is_useful (bool)
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if self.llm is None:
            self.load_model()

        results = []

        if self.verbose:
            print(f"\nüöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents_df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM...")
            iterator = tqdm(documents_df.iterrows(), total=len(documents_df), desc="LLM Cleaning")
        else:
            iterator = documents_df.iterrows()

        for idx, row in iterator:
            text = row[text_column]

            # –û—á–∏—â–∞–µ–º —á–µ—Ä–µ–∑ LLM
            cleaned = self.clean_document(text)

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ + –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            result_row = {
                **row.to_dict(),
                'clean_text': cleaned.get('clean_text', text),
                'products': json.dumps(cleaned.get('products', []), ensure_ascii=False),
                'actions': json.dumps(cleaned.get('actions', []), ensure_ascii=False),
                'conditions': json.dumps(cleaned.get('conditions', []), ensure_ascii=False),
                'topics': json.dumps(cleaned.get('topics', []), ensure_ascii=False),
                'usefulness_score': cleaned.get('usefulness_score', 0.5),
                'is_useful': cleaned.get('is_useful', True)
            }

            results.append(result_row)

        result_df = pd.DataFrame(results)

        if self.verbose:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            useful_count = result_df['is_useful'].sum()
            avg_score = result_df['usefulness_score'].mean()

            print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
            print(f"   –ü–æ–ª–µ–∑–Ω—ã—Ö: {useful_count}/{len(result_df)} ({useful_count/len(result_df)*100:.1f}%)")
            print(f"   –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.2f}")

            # –¢–æ–ø —Ç–µ–º—ã
            all_topics = []
            for topics_json in result_df['topics']:
                try:
                    topics = json.loads(topics_json)
                    all_topics.extend(topics)
                except:
                    pass

            if all_topics:
                from collections import Counter
                topic_counts = Counter(all_topics)

                print(f"\nüìä –¢–æ–ø-5 —Ç–µ–º:")
                for topic, count in topic_counts.most_common(5):
                    print(f"   {topic}: {count}")

        return result_df

    def filter_by_usefulness(self, documents_df: pd.DataFrame,
                            min_score: float = 0.3) -> pd.DataFrame:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏

        Args:
            documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–ø–æ—Å–ª–µ process_documents)
            min_score: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ usefulness_score

        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        before_count = len(documents_df)

        filtered_df = documents_df[
            (documents_df['is_useful'] == True) &
            (documents_df['usefulness_score'] >= min_score)
        ].copy()

        after_count = len(filtered_df)
        removed_count = before_count - after_count

        if self.verbose:
            print(f"\nüóëÔ∏è  –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (min_score={min_score}):")
            print(f"   –ë—ã–ª–æ: {before_count}")
            print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {after_count}")
            print(f"   –£–¥–∞–ª–µ–Ω–æ: {removed_count} ({removed_count/before_count*100:.1f}%)")

        return filtered_df


def apply_llm_cleaning(documents_df: pd.DataFrame,
                      min_usefulness: float = 0.3,
                      verbose: bool = True) -> pd.DataFrame:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM –æ—á–∏—Å—Ç–∫–∏

    Args:
        documents_df: DataFrame —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        min_usefulness: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π DataFrame
    """
    cleaner = LLMDocumentCleaner(verbose=verbose)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    cleaned_df = cleaner.process_documents(documents_df)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if min_usefulness > 0:
        cleaned_df = cleaner.filter_by_usefulness(cleaned_df, min_score=min_usefulness)

    return cleaned_df


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
    print("="*80)
    print("–¢–ï–°–¢ LLM DOCUMENT CLEANER")
    print("="*80)

    # –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    test_doc = """
    –ì–ª–∞–≤–Ω–∞—è / –ö–∞—Ä—Ç—ã / –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞

    –ê–ª—å—Ñ–∞-–ö–∞—Ä—Ç–∞ - –¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å –∫—ç—à–±—ç–∫–æ–º

    –ü–æ–ª—É—á–∞–π—Ç–µ –∫—ç—à–±—ç–∫ 2% –Ω–∞ –≤—Å–µ –ø–æ–∫—É–ø–∫–∏ –∏ –¥–æ 10% –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –Ω–∞ –≤—ã–±–æ—Ä.
    –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–∏ —Å—É–º–º–µ –ø–æ–∫—É–ø–æ–∫ –æ—Ç 10000 —Ä—É–±–ª–µ–π –≤ –º–µ—Å—è—Ü.

    –û—Ñ–æ—Ä–º–∏—Ç—å –æ–Ω–ª–∞–π–Ω

    ¬© 2001-2025 –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫. –õ–∏—Ü–µ–Ω–∑–∏—è –¶–ë –†–§ ‚Ññ1326
    """

    test_df = pd.DataFrame([{'text': test_doc, 'web_id': 1}])

    cleaner = LLMDocumentCleaner(verbose=True)
    result_df = cleaner.process_documents(test_df)

    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢:")
    print("="*80)
    print(f"\n–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{result_df.iloc[0]['clean_text']}")
    print(f"\n–ü—Ä–æ–¥—É–∫—Ç—ã: {result_df.iloc[0]['products']}")
    print(f"–¢–µ–º—ã: {result_df.iloc[0]['topics']}")
    print(f"–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: {result_df.iloc[0]['usefulness_score']}")
