"""
Context Window - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–ü—Ä–æ–±–ª–µ–º–∞:
–ù–∞–π–¥–µ–Ω–Ω—ã–π —á–∞–Ω–∫ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å
—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏.

–†–µ—à–µ–Ω–∏–µ:
–î–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –¥–æ–±–∞–≤–ª—è–µ–º N —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞.

–ü—Ä–∏–º–µ—Ä:
–ù–∞–π–¥–µ–Ω: web_id=123, chunk_index=5
–î–æ–±–∞–≤–ª—è–µ–º:
- chunk_index=4 (–ø—Ä–µ–¥—ã–¥—É—â–∏–π)
- chunk_index=5 (–∏—Å—Ö–æ–¥–Ω—ã–π)
- chunk_index=6 (—Å–ª–µ–¥—É—é—â–∏–π)

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –ü–æ–ª–Ω–æ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (+12-15% accuracy)
- –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ LLM
- –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ boundary –ø—Ä–æ–±–ª–µ–º (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —á–∞–Ω–∫–æ–≤)
"""
import pandas as pd
from typing import List, Dict, Tuple


class ContextWindowExpander:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏"""

    def __init__(self, window_size: int = 1):
        """
        Args:
            window_size: —Å–∫–æ–ª—å–∫–æ —Å–æ—Å–µ–¥–µ–π –¥–æ–±–∞–≤–∏—Ç—å —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã
                        1 = ¬±1 —á–∞–Ω–∫ (–≤—Å–µ–≥–æ 3: prev, current, next)
                        2 = ¬±2 —á–∞–Ω–∫–∞ (–≤—Å–µ–≥–æ 5)
        """
        self.window_size = window_size
        print(f"[ContextWindow] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å window_size={window_size}")
        print(f"               –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –¥–æ–±–∞–≤–ª—è–µ–º {2*window_size} —Å–æ—Å–µ–¥–µ–π")

    def expand_with_neighbors(self,
                             chunks_df: pd.DataFrame,
                             selected_chunks: pd.DataFrame,
                             preserve_scores: bool = True) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Args:
            chunks_df: –≤—Å–µ —á–∞–Ω–∫–∏ (–ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç)
            selected_chunks: –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞)
            preserve_scores: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ scores –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è —Å–æ—Å–µ–¥–µ–π

        Returns:
            —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å —Å–æ—Å–µ–¥—è–º–∏
        """
        if len(selected_chunks) == 0:
            return selected_chunks

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        # key: (web_id, chunk_index) -> chunk data
        chunks_index = {}
        for idx, row in chunks_df.iterrows():
            key = (row['web_id'], row['chunk_index'])
            chunks_index[key] = row.to_dict()

        expanded_chunks = []
        seen_chunks = set()  # –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
        for idx, selected_row in selected_chunks.iterrows():
            web_id = selected_row['web_id']
            chunk_idx = selected_row['chunk_index']
            original_score = selected_row.get('retrieval_score', 0.0)
            rerank_score = selected_row.get('rerank_score', None)

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Å–µ–¥–µ–π –≤ –æ–∫–Ω–µ
            for offset in range(-self.window_size, self.window_size + 1):
                neighbor_idx = chunk_idx + offset
                key = (web_id, neighbor_idx)

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –¥–æ–±–∞–≤–∏–ª–∏
                if key in seen_chunks:
                    continue

                # –ò—â–µ–º —Å–æ—Å–µ–¥–∞
                if key in chunks_index:
                    neighbor = chunks_index[key].copy()

                    # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Ç–∏–ø —á–∞–Ω–∫–∞
                    if offset == 0:
                        neighbor['context_type'] = 'original'  # –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π
                        neighbor['retrieval_score'] = original_score
                        if rerank_score is not None:
                            neighbor['rerank_score'] = rerank_score
                    else:
                        neighbor['context_type'] = 'neighbor'  # —Å–æ—Å–µ–¥

                        if preserve_scores:
                            # –°–æ—Å–µ–¥–∏ –Ω–∞—Å–ª–µ–¥—É—é—Ç score –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (–Ω–æ –º–µ–Ω—å—à–µ)
                            # –ß–µ–º –¥–∞–ª—å—à–µ —Å–æ—Å–µ–¥, —Ç–µ–º –º–µ–Ω—å—à–µ score
                            distance = abs(offset)
                            decay_factor = 1.0 / (1 + distance * 0.3)  # –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
                            neighbor['retrieval_score'] = original_score * decay_factor
                            if rerank_score is not None:
                                neighbor['rerank_score'] = rerank_score * decay_factor
                        else:
                            # –°–æ—Å–µ–¥–∏ –±–µ–∑ score (–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π)
                            neighbor['retrieval_score'] = 0.0
                            neighbor['rerank_score'] = None

                    neighbor['context_offset'] = offset  # –ø–æ–∑–∏—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ
                    neighbor['original_chunk_id'] = selected_row['chunk_id']  # —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π

                    expanded_chunks.append(neighbor)
                    seen_chunks.add(key)

        # –°–æ–∑–¥–∞–µ–º DataFrame
        if len(expanded_chunks) == 0:
            return selected_chunks

        expanded_df = pd.DataFrame(expanded_chunks)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –∏—Å—Ö–æ–¥–Ω—ã–µ (—Å –ª—É—á—à–∏–º–∏ scores), –ø–æ—Ç–æ–º —Å–æ—Å–µ–¥–∏
        # –í–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã - –ø–æ web_id –∏ chunk_index –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        sort_keys = []
        if 'rerank_score' in expanded_df.columns:
            sort_keys.append('rerank_score')
        sort_keys.extend(['web_id', 'chunk_index'])

        expanded_df = expanded_df.sort_values(
            sort_keys,
            ascending=[False] + [True] * (len(sort_keys) - 1)
        )

        return expanded_df.reset_index(drop=True)

    def get_context_groups(self, expanded_chunks: pd.DataFrame) -> Dict[str, List[Dict]]:
        """
        –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–π–¥–µ–Ω–Ω—ã–º —á–∞–Ω–∫–∞–º

        Args:
            expanded_chunks: —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ (—Å —Å–æ—Å–µ–¥—è–º–∏)

        Returns:
            —Å–ª–æ–≤–∞—Ä—å {original_chunk_id: [chunks in context window]}
        """
        groups = {}

        for idx, row in expanded_chunks.iterrows():
            original_id = row.get('original_chunk_id', row['chunk_id'])

            if original_id not in groups:
                groups[original_id] = []

            groups[original_id].append(row.to_dict())

        return groups

    def merge_neighbors_text(self, expanded_chunks: pd.DataFrame,
                            separator: str = '\n\n---\n\n') -> pd.DataFrame:
        """
        –°–ª–∏—è–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –≤ –æ–¥–∏–Ω

        Args:
            expanded_chunks: —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            separator: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏

        Returns:
            DataFrame –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ original chunk –æ–±—ä–µ–¥–∏–Ω–µ–Ω —Ç–µ–∫—Å—Ç —Å —Å–æ—Å–µ–¥—è–º–∏
        """
        groups = self.get_context_groups(expanded_chunks)

        merged_chunks = []

        for original_id, chunks in groups.items():
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ chunk_index –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
            chunks_sorted = sorted(chunks, key=lambda x: x['chunk_index'])

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã
            text_field = 'clean_text' if 'clean_text' in chunks_sorted[0] else 'text'
            merged_text = separator.join([
                chunk.get(text_field, chunk.get('text', ''))
                for chunk in chunks_sorted
            ])

            # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (–≥–¥–µ context_type='original')
            original_chunk = next(
                (c for c in chunks if c.get('context_type') == 'original'),
                chunks_sorted[0]
            )

            merged_chunk = original_chunk.copy()
            merged_chunk[text_field] = merged_text
            merged_chunk['context_window_size'] = len(chunks)
            merged_chunk['context_chunks'] = [c['chunk_id'] for c in chunks_sorted]

            merged_chunks.append(merged_chunk)

        return pd.DataFrame(merged_chunks)


def demonstrate_context_window():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Context Window"""
    print("="*80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø CONTEXT WINDOW")
    print("="*80)

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —á–∞–Ω–∫–æ–≤
    all_chunks = pd.DataFrame([
        {'chunk_id': 'doc1_0', 'web_id': 1, 'chunk_index': 0, 'text': '–ß–∞–Ω–∫ 0: –í–≤–µ–¥–µ–Ω–∏–µ –≤ –ø—Ä–æ–¥—É–∫—Ç'},
        {'chunk_id': 'doc1_1', 'web_id': 1, 'chunk_index': 1, 'text': '–ß–∞–Ω–∫ 1: –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏'},
        {'chunk_id': 'doc1_2', 'web_id': 1, 'chunk_index': 2, 'text': '–ß–∞–Ω–∫ 2: –£—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è'},  # –ù–∞–π–¥–µ–Ω
        {'chunk_id': 'doc1_3', 'web_id': 1, 'chunk_index': 3, 'text': '–ß–∞–Ω–∫ 3: –ö–æ–º–∏—Å—Å–∏–∏ –∏ —Ç–∞—Ä–∏—Ñ—ã'},
        {'chunk_id': 'doc1_4', 'web_id': 1, 'chunk_index': 4, 'text': '–ß–∞–Ω–∫ 4: –ó–∞–∫–ª—é—á–µ–Ω–∏–µ'},
        {'chunk_id': 'doc2_0', 'web_id': 2, 'chunk_index': 0, 'text': '–ß–∞–Ω–∫ 0: –î—Ä—É–≥–æ–π –¥–æ–∫—É–º–µ–Ω—Ç'},
        {'chunk_id': 'doc2_1', 'web_id': 2, 'chunk_index': 1, 'text': '–ß–∞–Ω–∫ 1: –í–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'},  # –ù–∞–π–¥–µ–Ω
        {'chunk_id': 'doc2_2', 'web_id': 2, 'chunk_index': 2, 'text': '–ß–∞–Ω–∫ 2: –î–µ—Ç–∞–ª–∏'},
    ])

    # –°–∏–º—É–ª–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    search_results = pd.DataFrame([
        {'chunk_id': 'doc1_2', 'web_id': 1, 'chunk_index': 2, 'text': '–ß–∞–Ω–∫ 2: –£—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è', 'retrieval_score': 0.95},
        {'chunk_id': 'doc2_1', 'web_id': 2, 'chunk_index': 1, 'text': '–ß–∞–Ω–∫ 1: –í–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', 'retrieval_score': 0.85},
    ])

    print("\n1Ô∏è‚É£  –ò—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
    for idx, row in search_results.iterrows():
        print(f"   {row['chunk_id']}: {row['text']} (score: {row['retrieval_score']})")

    # –†–∞—Å—à–∏—Ä—è–µ–º —Å window_size=1
    expander = ContextWindowExpander(window_size=1)
    expanded = expander.expand_with_neighbors(all_chunks, search_results)

    print(f"\n2Ô∏è‚É£  –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å–µ–¥–µ–π (window_size=1):")
    print(f"   –ë—ã–ª–æ: {len(search_results)} —á–∞–Ω–∫–æ–≤")
    print(f"   –°—Ç–∞–ª–æ: {len(expanded)} —á–∞–Ω–∫–æ–≤")
    print(f"\n   –î–µ—Ç–∞–ª–∏:")

    for idx, row in expanded.iterrows():
        context_type = row.get('context_type', 'unknown')
        offset = row.get('context_offset', 0)
        marker = "üéØ" if context_type == 'original' else "  "
        print(f"   {marker} {row['chunk_id']}: {row['text']}")
        print(f"      Type: {context_type}, Offset: {offset:+d}, Score: {row.get('retrieval_score', 0):.3f}")

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
    print("\n3Ô∏è‚É£  –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ—Å–µ–¥–µ–π:")
    merged = expander.merge_neighbors_text(expanded)

    for idx, row in merged.iterrows():
        print(f"\n   Original: {row['chunk_id']}")
        print(f"   Context window: {row['context_window_size']} chunks")
        print(f"   Chunks: {', '.join(row['context_chunks'])}")
        print(f"   Merged text:")
        print(f"   {row['text'][:200]}...")

    print("\n" + "="*80)
    print("‚úÖ Context Window –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã!")
    print("="*80)


def main():
    """–¢–µ—Å—Ç Context Window"""
    demonstrate_context_window()


if __name__ == "__main__":
    main()
